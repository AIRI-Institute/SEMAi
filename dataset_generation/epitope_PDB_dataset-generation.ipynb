{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Fetch PDB sequences and generate blast DB <br>\n",
    "\n",
    "Each PDB formatted file includes \"SEQRES records\" which list the primary sequence of the polymeric molecules present in the entry. This sequence information is also available as a FASTA download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from Bio.Blast import NCBIXML\n",
    "from pathlib import Path\n",
    "import sys \n",
    "\n",
    "if not os.path.exists(\"PDB_blast_db\"):\n",
    "    os.mkdir(\"PDB_blast_db\")\n",
    "\n",
    "if not os.path.exists(\"PDB_blast_db/pdb_seqres.txt\"):\n",
    "    subprocess.call(\"wget http://ftp.wwpdb.org/pub/pdb/derived_data/pdb_seqres.txt.gz\",shell=True)    \n",
    "    subprocess.call(\"gzip -d pdb_seqres.txt.gz\",shell=True)\n",
    "    subprocess.call(\"mv pdb_seqres.txt ./PDB_blast_db/\",shell=True)\n",
    "    \n",
    "if not os.path.exists(\"PDB_blast_db/pdb_seqres.txt.psq\"):\n",
    "    subprocess.call(\"makeblastdb -in pdb_seqres.txt -dbtype prot -title pdb\",shell=True, cwd=\"./PDB_blast_db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preliminary screen for proteins in the PDB database with homology to the random human fragment antigen-binding region "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch PDB's with Fab's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "light = \"DILLTQSPVILSVSPGERVSFSCRASQSIGTNIHWYQQRTNGSPRLLIKYASESISGIPSRFSGSGSGTDFTLSINSVESEDIADYYCQQNNNWPTTFGAGTKLELK\"\n",
    "with open(\"PDB_blast_db/fab_light.fasta\",'w') as fo:\n",
    "    fo.write(\">input_light\\n\")\n",
    "    fo.write(light)\n",
    "\n",
    "heavy = \"QVQLKQSGPGLVQPSQSLSITCTVSGFSLTNYGVHWVRQSPGKGLEWLGVIWSGGNTDYNTPFTSRLSINKDNSKSQVFFKMNSLQSNDTAIYYCARALTYYDYEFAYWGQGTLVTVSA\"\n",
    "with open(\"PDB_blast_db/fab_heavy.fasta\",'w') as fo:\n",
    "    fo.write(\">input_heavy\\n\")\n",
    "    fo.write(heavy)\n",
    "\n",
    "if not os.path.exists(\"PDB_blast_db/hits_fabs_light.xml\"):\n",
    "    subprocess.call(\"blastp -db pdb_seqres.txt -num_alignments 99999 -evalue 1e-9 -query fabs.fasta -out hits_fabs_light.xml -outfmt 5\",shell=True, cwd=\"./PDB_blast_db\")\n",
    "\n",
    "if not os.path.exists(\"PDB_blast_db/hits_fabs_heavy.xml\"):\n",
    "    subprocess.call(\"blastp -db pdb_seqres.txt -num_alignments 99999 -evalue 1e-9 -query fabs.fasta -out hits_fabs_heavy.xml -outfmt 5\",shell=True, cwd=\"./PDB_blast_db\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractPDBIDsfromBlastOutput(input_path): \n",
    "    result=open(input_path,\"r\")\n",
    "    records= NCBIXML.parse(result)\n",
    "    item=next(records)\n",
    "    pdb_fabs  = set()\n",
    "    pdb_fabs_ = set()\n",
    "    for alignment in item.alignments:\n",
    "        #print(alignment)\n",
    "        #break\n",
    "        for hsp in alignment.hsps:\n",
    "            pdb_id = alignment.title.split()[1]\n",
    "            pdb_id_id = pdb_id.split(\"_\")[0]\n",
    "            pdb_fabs.add(pdb_id)\n",
    "            pdb_fabs_.add(pdb_id_id)\n",
    "    return pdb_fabs\n",
    "pdb_fab_hits_1 =  extractPDBIDsfromBlastOutput(\"PDB_blast_db/hits_fabs_light.xml\")\n",
    "pdb_fab_hits_2 =  extractPDBIDsfromBlastOutput(\"PDB_blast_db/hits_fabs_heavy.xml\")\n",
    "pdb_fab_hits   = pdb_fab_hits_1|pdb_fab_hits_2\n",
    "\n",
    "print(len(pdb_fab_hits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Screen for heavy and light fab chains using ANARCI\n",
    "\n",
    "http://opig.stats.ox.ac.uk/webapps/newsabdab/sabpred/anarci/ <br>\n",
    "https://github.com/oxpig/ANARCI\n",
    "\n",
    "Annotate all Fabs with ANARCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFasta(path):\n",
    "    r = []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            if line[0]==\">\":\n",
    "                r.append([])\n",
    "            r[-1].append(line.rstrip())\n",
    "    r = [[r_[0],\"\".join(r_[1:])] for r_ in r]\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify and annotate Fab's from the blast output results using ANARCI tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = []\n",
    "with open(\"./PDB_blast_db/pdb_seqres.txt\") as f:\n",
    "    for line in f:\n",
    "        if line[0]==\">\":\n",
    "            r.append([])\n",
    "        r[-1].append(line)\n",
    "        \n",
    "pdb_seqres_fasta = loadFasta(\"./PDB_blast_db/pdb_seqres.txt\")\n",
    "\n",
    "rfabs = []\n",
    "for r_ in r:\n",
    "    title = r_[0].split(\" \")[0][1:]\n",
    "    if title not in pdb_fab_hits:\n",
    "        continue\n",
    "    rfabs.append([r_[0].split(\" \")[0][1:],r_[1]])\n",
    "    \n",
    "with open(\"./PDB_blast_db/putative_fabs.fasta\",'w') as fo:\n",
    "    for r in rfabs:\n",
    "        fo.write(\"\".join([\">\"+r[0]+\"\\n\",r[1]])+\"\\n\")\n",
    "\n",
    "if not os.path.exists(\"./PDB_blast_db/all_fabs_heavy.anarci\"):\n",
    "    anarci_command = \"ANARCI -i putative_fabs.fasta -o all_fabs_heavy.anarci -s chothia -r ig --ncpu 8 --bit_score_threshold 100 --restrict heavy\"\n",
    "    subprocess.call(anarci_command,shell=True,cwd=\"./PDB_blast_db\")\n",
    "    \n",
    "if not os.path.exists(\"./PDB_blast_db/all_fabs_light.anarci\"):\n",
    "    anarci_command = \"ANARCI -i putative_fabs.fasta -o all_fabs_light.anarci -s chothia -r ig --ncpu 8 --bit_score_threshold 100 --restrict light\"\n",
    "    subprocess.call(anarci_command,shell=True,cwd=\"./PDB_blast_db\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./PDB_blast_db/all_fabs_heavy.anarci\"):\n",
    "    anarci_command = \"/mnt/10tb/nvivanisenko/anaconda3/envs/folding/bin/ANARCI -i putative_fabs.fasta -o all_fabs_heavy.anarci -s chothia -r ig --ncpu 8 --bit_score_threshold 100 --restrict heavy\"\n",
    "    subprocess.call(anarci_command,shell=True,cwd=\"./PDB_blast_db\")\n",
    "    \n",
    "if not os.path.exists(\"./PDB_blast_db/all_fabs_light.anarci\"):\n",
    "    anarci_command = \"/mnt/10tb/nvivanisenko/anaconda3/envs/folding/bin/ANARCI -i putative_fabs.fasta -o all_fabs_light.anarci -s chothia -r ig --ncpu 8 --bit_score_threshold 100 --restrict light\"\n",
    "    subprocess.call(anarci_command,shell=True,cwd=\"./PDB_blast_db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Parse ANARCI output and extract heavy and light Fab sequences \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseAnarciAnnotation(path = \"light.anarci\",n=108):\n",
    "    seqs = []\n",
    "    seqs.append([[] for i in range(n)])\n",
    "    used = set()\n",
    "    data = {}\n",
    "    with open(path) as f:\n",
    "        w = f.readlines()\n",
    "        data = [[]]\n",
    "        for u,line in enumerate(w):\n",
    "            data[-1].append(line)\n",
    "            if line[0]==\"/\":\n",
    "                data.append([])          \n",
    "        out = {}\n",
    "        for d in data:\n",
    "            if len(d)==0:\n",
    "                continue\n",
    "            name = d[0].rstrip().split()[-1]\n",
    "            if name in out:\n",
    "                continue\n",
    "            out[name] = [[] for i in range(n)]            \n",
    "            for d_ in d:\n",
    "                if d_[0]==\"#\":\n",
    "                    continue\n",
    "                if d_[0]==\"/\":\n",
    "                    continue\n",
    "                id_ = d_.split()[1]\n",
    "                id_ = int(id_)\n",
    "                if d_[10]==\"-\":\n",
    "                    continue\n",
    "                out[name][id_].append(d_[10])\n",
    "    out_ = {}\n",
    "    for name in out:\n",
    "        if len(\"\".join([\"\".join(c) for c in out[name]]))==0:\n",
    "            continue\n",
    "        out_[name] = out[name]\n",
    "    return out_\n",
    "\n",
    "anarci_list_heavy = parseAnarciAnnotation(\"./PDB_blast_db/all_fabs_heavy.anarci\",n=120)\n",
    "anarci_list_light = parseAnarciAnnotation(\"./PDB_blast_db/all_fabs_light.anarci\",n=108)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Fetch all PDB structures containing Light/Heavy chains "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pdb_3 = {r[:4]:{\"light\":[],\"heavy\":[]} for r in list(pdb_fabs)}\n",
    "\n",
    "for h in anarci_list_light:\n",
    "    h4 = h[:4]\n",
    "    if h4 not in pdb_3:\n",
    "        continue\n",
    "    pdb_3[h4][\"light\"].append(h)\n",
    "    \n",
    "for h in anarci_list_heavy:\n",
    "    h4 = h[:4]\n",
    "    if h4 not in pdb_3:\n",
    "        continue\n",
    "    pdb_3[h4][\"heavy\"].append(h)\n",
    "    \n",
    "if not os.path.exists(\"PDB_blast_db/structs\"):\n",
    "    os.mkdir(\"PDB_blast_db/structs\")\n",
    "\n",
    "for pdb_ in pdb_3:\n",
    "    if len(pdb_3[pdb_][\"light\"])+len(pdb_3[pdb_][\"heavy\"])==0:\n",
    "        continue    \n",
    "    pdb_name = pdb_.upper()+\".pdb.gz\"\n",
    "    if os.path.exists(\"PDB_blast_db/structs/\"+pdb_name):\n",
    "        continue\n",
    "    if os.path.exists(\"PDB_blast_db/structs/\"+pdb_name.rstrip(\".gz\")):\n",
    "        continue\n",
    "    subprocess.call(f\"wget https://files.rcsb.org/download/{pdb_name}\",shell=True, cwd=\"./PDB_blast_db/structs\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep only first model for multimodel PDB structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "pdbs = Path(\"./PDB_blast_db/structs/\").glob(\"*.pdb\")\n",
    "for pdb in pdbs:\n",
    "    print(pdb)\n",
    "    pdb_data = pdb.open('r').readlines()\n",
    "    fo = open(str(pdb),'w')#.open('w')\n",
    "    for l in pdb_data:\n",
    "        fo.write(l)\n",
    "        if l.startswith(\"ENDMDL\"):\n",
    "            break\n",
    "    fo.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Prepare PDB dataframes and align full sequence (from pdb seq-res) on sequence of resloved protein (may contain some gaps) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLetterFromResidueName(resn):\n",
    "    d = {'CYS': 'C', 'ASP': 'D', 'SER': 'S', 'GLN': 'Q', 'LYS': 'K',\n",
    "     'ILE': 'I', 'PRO': 'P', 'THR': 'T', 'PHE': 'F', 'ASN': 'N',\n",
    "     'GLY': 'G', 'HIS': 'H', 'LEU': 'L', 'ARG': 'R', 'TRP': 'W',\n",
    "     'ALA': 'A', 'VAL':'V', 'GLU': 'E', 'TYR': 'Y', 'MET': 'M'}\n",
    "    return d[resn]\n",
    "\n",
    "def kalign(seq1,seq2):\n",
    "    if not os.path.exists(\"./PDB_blast_db/temp\"):\n",
    "        os.mkdir(\"./PDB_blast_db/temp\")\n",
    "    fo = open(\"./PDB_blast_db/temp/input.fasta\",'w')\n",
    "    fo.write(f\">1\\n{seq1}\\n>2\\n{seq2}\\n\")\n",
    "    fo.close()\n",
    "    d = subprocess.check_output(\"cat ./PDB_blast_db/temp/input.fasta | kalign -f fasta\",shell=True)\n",
    "    res_  = d.decode(\"UTF-8\").split(\"\\n\")\n",
    "    res   = []\n",
    "    for l in res_:\n",
    "        if len(l) ==0:\n",
    "            continue\n",
    "        if l[0]==\">\":\n",
    "            res.append([])\n",
    "        if len(res)!=0:\n",
    "            res[-1].append(l.rstrip(\"\\n\"))\n",
    "    return \"\".join(res[0][1:]),\"\".join(res[1][1:])\n",
    "    \n",
    "    \n",
    "def removeAlternativeConformations(pdb_dataframe):\n",
    "    return pdb_dataframe[(pdb_dataframe[\"alt_loc\"] == \"A\") | (pdb_dataframe[\"alt_loc\"] == \" \")  | (pdb_dataframe[\"alt_loc\"] == \"\")]\n",
    "    \n",
    "def removeUNK(pdb_dataframe):\n",
    "    r = pdb_dataframe[\"residue_name\"] == \"UNK\"\n",
    "    return pdb_dataframe[~r]\n",
    "\n",
    "def considerInsertions(pdb_dataframe):\n",
    "    r1 = pdb_dataframe[\"residue_number\"]\n",
    "    r2 = pdb_dataframe[\"insertion\"]\n",
    "    r3 = pdb_dataframe[\"chain_id\"]\n",
    "    r4 = pdb_dataframe[\"residue_name\"]\n",
    "    ra = [(r_1,r_2,r_3,r_4) for (r_1,r_2,r_3,r_4) in zip(r1,r2,r3,r4)]\n",
    "    pdb_dataframe[\"residue_key\"] = ra\n",
    "    return pdb_dataframe\n",
    "    \n",
    "def putFullSequence(pdb_dataframe, full_seq):\n",
    "    pdb_dataframe = removeAlternativeConformations(pdb_dataframe)\n",
    "    pdb_dataframe = removeUNK(pdb_dataframe)\n",
    "    pdb_dataframe = considerInsertions(pdb_dataframe)\n",
    "    \n",
    "    if len(pdb_dataframe) == 0:\n",
    "        print(\"Empty...\")\n",
    "        return\n",
    "    pdb_ca                          = pdb_dataframe[pdb_dataframe[\"atom_name\"] == \"CA\"]    \n",
    "    residue_numbers = []\n",
    "    residue_seq     = []\n",
    "    used = set()\n",
    "    \n",
    "    #### To avoid residue duplicates \n",
    "    #### Might be not necessary\n",
    "    for r in pdb_ca.iloc():#[\"residue_number\"]:\n",
    "        residue_number = r[\"residue_key\"]\n",
    "        if residue_number in used:\n",
    "            continue\n",
    "        residue_numbers.append(residue_number)\n",
    "        residue_seq.append(    getLetterFromResidueName(r[\"residue_name\"]))\n",
    "    \n",
    "    pdb_seq = \"\".join(residue_seq)#pdb_seq)    \n",
    "    if len(pdb_seq) <= 5:\n",
    "        print(\"PDB sequence is too short\")\n",
    "        return\n",
    "    pdb_seq_aligned, full_seq_aligned = kalign(pdb_seq,full_seq)\n",
    "\n",
    "    assert full_seq_aligned.replace(\"-\",\"\") == full_seq\n",
    "    \n",
    "    n_pdb     = -1\n",
    "    n_pdb_map = []\n",
    "    \n",
    "    print(\"pdb\",pdb_seq_aligned)\n",
    "    print(\"full\",full_seq_aligned)\n",
    "    \n",
    "    assert len(pdb_seq_aligned) == len(full_seq_aligned)\n",
    "    \n",
    "    new_dataframe = []\n",
    "    for i,[a_pdb,a_fullseq] in enumerate(zip(list(pdb_seq_aligned),list(full_seq_aligned))):\n",
    "        if a_pdb != '-':\n",
    "            n_pdb+=1\n",
    "        if a_pdb == \"-\":\n",
    "            n_pdb_map.append({\"resi\":None,                  \"a_pdb\":None, \"a_full\":a_fullseq})\n",
    "        else:\n",
    "            n_pdb_map.append({\"resi\":residue_numbers[n_pdb],\"a_pdb\":a_pdb,\"a_full\":a_fullseq})\n",
    "\n",
    "    full_df  = []\n",
    "    full_seq = []\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "    \n",
    "    for r in n_pdb_map:\n",
    "        full_seq.append(r[\"a_full\"])\n",
    "        \n",
    "        if r[\"resi\"] is None:\n",
    "            empty_   = pd.DataFrame(np.nan, index=[0],columns=pdb_ca.columns)\n",
    "            empty_[\"atom_name\"] = \"CA\"\n",
    "            empty_[\"seqres\"]    = r[\"a_full\"]\n",
    "            full_df.append(empty_)\n",
    "            continue\n",
    "        \n",
    "        pdb_residue = pdb_dataframe[pdb_dataframe[\"residue_key\"] == r[\"resi\"]]\n",
    "        pdb_residue[\"seqres\"] = r[\"a_full\"]\n",
    "        pdb_residue[\"aa\"]     = r[\"a_pdb\"]\n",
    "        full_df.append(pdb_residue)  \n",
    "\n",
    "    full_df = pd.concat(full_df,axis=0,ignore_index=True)\n",
    "    return full_df\n",
    "\n",
    "def getJobs():\n",
    "    if not os.path.exists(\"./PDB_blast_db/all_pdbids_and_chains.txt\"):\n",
    "        all_chains = set()\n",
    "        p = Path(\"./PDB_blast_db/structs/\").glob(\"*.pdb\")    \n",
    "        for p_ in p:\n",
    "            for line in p_.open('r'):\n",
    "                if line.startswith(\"ATOM\") and line[13:15] ==\"CA\":\n",
    "                    all_chains.add(p_.name.rstrip(\".pdb\")+\"_\"+line[21])                \n",
    "        with open(\"./PDB_blast_db/all_pdbids_and_chains.txt\",'w') as fo:\n",
    "            fo.write(\"\\n\".join(list(all_chains)))\n",
    "\n",
    "    all_chains = [r.rstrip() for r in open(\"./PDB_blast_db/all_pdbids_and_chains.txt\",'r').readlines()]\n",
    "    used = set()\n",
    "    for p in Path(\"./PDB_blast_db/structs_per_chain/\").glob(\"*.pkl\"):\n",
    "        used.add(p.name.rstrip(\".pkl\"))\n",
    "    j = {}\n",
    "    for u in all_chains:\n",
    "        if u in used:\n",
    "            continue\n",
    "        pdbid,chain = u.split(\"_\")\n",
    "        j.setdefault(pdbid,set())\n",
    "        j[pdbid].add(chain)\n",
    "    return j\n",
    "\n",
    "\n",
    "\n",
    "def getPDBDataFrame(pdb_id = \"1FGV\",chains = None):   \n",
    "    pdb_path           = f\"./PDB_blast_db/structs/{pdb_id.upper()}.pdb\"\n",
    "    if not os.path.exists(pdb_path):\n",
    "        print(pdb_id,\" not found\")\n",
    "        return\n",
    "    pdb_structure      = PandasPdb().read_pdb(pdb_path).df[\"ATOM\"]\n",
    "    \n",
    "    from Bio import SeqIO\n",
    "    \n",
    "    sequences = {}\n",
    "    pdb_records = {}\n",
    "    \n",
    "    if not os.path.exists(f\"./PDB_blast_db/structs_per_chain/\"):\n",
    "        os.mkdir(f\"./PDB_blast_db/structs_per_chain/\")\n",
    "\n",
    "    for record in SeqIO.parse(pdb_path, \"pdb-seqres\"):\n",
    "        chain             = record.id[-1]            \n",
    "        if chains is not None and chain not in chains:\n",
    "            continue\n",
    "        sequences[chain]  = record.seq\n",
    "        pdb_chain         = pdb_structure[pdb_structure[\"chain_id\"] == chain]\n",
    "        if len(pdb_chain) == 0:\n",
    "            continue        \n",
    "        print(pdb_id,chain)\n",
    "        pdb_chain_fullseq = putFullSequence(pdb_chain,sequences[chain])\n",
    "        pickle.dump(pdb_chain_fullseq, open(f\"./PDB_blast_db/structs_per_chain/{pdb_id}_{chain}.pkl\",'wb'))\n",
    "\n",
    "jobs = getJobs()\n",
    "\n",
    "for pdb_id in getJobs():\n",
    "    chains = jobs[pdb_id]\n",
    "    getPDBDataFrame(pdb_id,chains)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Put ANARCI annotation into antibodies dataframes prepared in the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mafft_align(s1,s2,strict=True):\n",
    "    with open(\"m.fasta\",'w') as fo:\n",
    "        fo.write(\">1\\n\"+s1+\"\\n>2\\n\"+s2+\"\\n\")\n",
    "    if not strict:\n",
    "        d = subprocess.check_output(\"mafft --anysymbol --op 0.1  m.fasta \",shell=True)\n",
    "    else:\n",
    "        d = subprocess.check_output(\"mafft --anysymbol --auto m.fasta \",shell=True)\n",
    "\n",
    "    res_  = d.decode(\"UTF-8\").split(\"\\n\")\n",
    "    res   = []\n",
    "    for l in res_:\n",
    "        if len(l)==0:\n",
    "            continue\n",
    "        if l[0]==\">\":\n",
    "            res.append(\"\")\n",
    "            continue\n",
    "        res[-1]+=l.rstrip()\n",
    "    return res\n",
    "\n",
    "def realignSequences(pdb_seq,anarci_, firstLetterException = False):\n",
    "    seq_aa = []\n",
    "    seq_i  = []\n",
    "    for i,s_ in enumerate(anarci_):\n",
    "        if len(s_)==0:\n",
    "            continue\n",
    "        seq_aa+=s_\n",
    "        seq_i +=[i for i_ in range(len(s_))]\n",
    "    al = kalign(\"\".join(seq_aa),\"\".join(pdb_seq))\n",
    "    seq_anarci_aligned = al[0]\n",
    "    pdb_seq_aligned    = al[1]\n",
    "    n_anarci = 0\n",
    "    n_pdb    = 0\n",
    "    pdb_anarci_map = [None for i in pdb_seq] \n",
    "    \n",
    "    for i,[a_anarci,a_pdb] in enumerate(zip(*al)):\n",
    "        if a_anarci!=\"-\" and a_pdb!=\"-\":\n",
    "            pdb_anarci_map[n_pdb] = i\n",
    "            if n_anarci == 0 and firstLetterException:\n",
    "                n_pdb+=1\n",
    "                n_anarci+=1\n",
    "                continue\n",
    "            if a_pdb!=a_anarci:\n",
    "                return None\n",
    "        if a_pdb!=\"-\":\n",
    "            n_pdb+=1\n",
    "        if a_anarci!=\"-\":            \n",
    "            n_anarci+=1\n",
    "            \n",
    "    return pdb_anarci_map\n",
    "    \n",
    "def putAnarciAnnotation(pdb_dataframe,fab_id, firstLetterException = False):\n",
    "    pdb_id,chain,fab_type = fab_id\n",
    "    if fab_type == \"light\":\n",
    "        anarci_seq     = anarci_list_light[pdb_id.lower()+\"_\"+chain]\n",
    "    else:\n",
    "        anarci_seq     = anarci_list_heavy[pdb_id.lower()+\"_\"+chain]\n",
    "    pdb_ca        = pdb_dataframe[pdb_dataframe[\"atom_name\"] == \"CA\"]\n",
    "    pdb_seq = \"\".join(pdb_ca[\"seqres\"])\n",
    "    pdb_anarci_map = realignSequences(pdb_seq, anarci_seq,firstLetterException)\n",
    "    if pdb_anarci_map is None:\n",
    "        return None\n",
    "    pdb_anarci_map = [fab_type[0].upper()+str(i)  if i is not None else None for i in pdb_anarci_map]    \n",
    "    pdb_dataframe[\"anarci\"] = None\n",
    "    for anarci_id,residue_number in zip(pdb_anarci_map, pdb_ca[\"residue_key\"]):\n",
    "        ids = pdb_dataframe[ \"residue_key\"] == residue_number\n",
    "        pdb_dataframe.loc[ids,\"anarci\"] = anarci_id     \n",
    "    return pdb_dataframe\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  An example of annotated Fab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_id, chain, fab_type = (\"7LYW\",\"H\",\"heavy\")\n",
    "pdb_id, chain, fab_type = (\"5HCG\",\"H\",\"heavy\")\n",
    "pdb_path           = f\"./PDB_blast_db/structs_per_chain/{pdb_id}_{chain}.pkl\"\n",
    "out_path           = f\"./PDB_blast_db/structs_antibodies/{pdb_id}_{chain}_{fab_type}.pkl\"\n",
    "fab                = pickle.load(open(pdb_path,'rb'))\n",
    "fab_annotated      = putAnarciAnnotation(fab, (pdb_id,chain,fab_type), firstLetterException=True)\n",
    "print(fab_annotated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f\"./PDB_blast_db/structs_antibodies/\"):\n",
    "    os.mkdir(f\"./PDB_blast_db/structs_antibodies/\")\n",
    "    \n",
    "    \n",
    "jobs = []\n",
    "for anarci_id,anarci_map in anarci_list_heavy.items():    \n",
    "    jobs.append((anarci_id[:4].upper(),anarci_id[-1],\"heavy\"))\n",
    "for anarci_id,anarci_map in anarci_list_light.items():    \n",
    "    jobs.append((anarci_id[:4].upper(),anarci_id[-1],\"light\"))\n",
    "\n",
    "strange_error_list = set()\n",
    "\n",
    "for pdb_id,chain,fab_type in jobs:\n",
    "    if pdb_id.upper()+\"_\"+chain in strange_error_list:\n",
    "        continue        \n",
    "    firstLetterException = True\n",
    "    print(pdb_id,chain,fab_type)\n",
    "\n",
    "    pdb_path           = f\"./PDB_blast_db/structs_per_chain/{pdb_id}_{chain}.pkl\"\n",
    "    if not os.path.exists(pdb_path):\n",
    "        continue\n",
    "        \n",
    "    out_path           = f\"./PDB_blast_db/structs_antibodies/{pdb_id}_{chain}_{fab_type}.pkl\"\n",
    "    if os.path.exists(out_path):\n",
    "        continue\n",
    "    fab                = pickle.load(open(pdb_path,'rb'))\n",
    "    fab_annotated      = putAnarciAnnotation(fab, (pdb_id,chain,fab_type),firstLetterException)\n",
    "    if fab_annotated is None:\n",
    "        strange_error_list.add((pdb_id,chain,fab_type))\n",
    "        continue            \n",
    "    pickle.dump(fab_annotated, open(out_path,'wb'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following structures got troubles in annotation and were ignored later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in strange_error_list:\n",
    "    print(e)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Identify Fab pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getPDBList():\n",
    "    return [p.name[:4] for p in Path(\"./PDB_blast_db/structs_antibodies/\").glob(f\"*.pkl\")]\n",
    "    \n",
    "def getFabsPDBID(pdb_id = \"1LK3\"):\n",
    "    fab_path = Path(\"./PDB_blast_db/structs_antibodies/\").glob(f\"{pdb_id}*.pkl\")\n",
    "    fab_ids  = {\"heavy\":[],\"light\":[]}\n",
    "    for struct_id in fab_path:\n",
    "        pdb_id, chain, fab_type = struct_id.name.rstrip(\".pkl\").split(\"_\")\n",
    "        fab_ids[fab_type].append(struct_id)\n",
    "    return fab_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.spatial import distance\n",
    "def getxyz(df):\n",
    "    xyz = np.array([df[\"x_coord\"],df[\"y_coord\"],df[\"z_coord\"]]).T\n",
    "    return xyz\n",
    "    \n",
    "    \n",
    "def getPairInterface(path_light, path_heavy):\n",
    "    \n",
    "    pdb_light = pickle.load(open(path_light,'rb'))\n",
    "    pdb_heavy = pickle.load(open(path_heavy,'rb'))\n",
    "    \n",
    "    heavy_interface = list(range(32,39)) + list(range(44,50)) + list(range(85,95))\n",
    "    light_interface = list(range(34,39)) + list(range(45,51)) + list(range(90,108))\n",
    "\n",
    "    heavy_ids = [\"H\"+str(i) for i in heavy_interface]\n",
    "    light_ids = [\"L\"+str(i) for i in heavy_interface]\n",
    "    \n",
    "    heavy_interface = pdb_heavy[pdb_heavy[\"anarci\"].isin(heavy_ids)]\n",
    "    light_interface = pdb_light[pdb_light[\"anarci\"].isin(light_ids)]\n",
    "    \n",
    "    xyz_heavy = getxyz(heavy_interface)\n",
    "    xyz_light = getxyz(light_interface)\n",
    "    cd = distance.cdist(xyz_heavy,xyz_light)\n",
    "    ids = np.where(cd<4.5)\n",
    "    \n",
    "    return len(set(ids[0]))+len(set(ids[1]))\n",
    "    \n",
    "def screenFabPairs(pdb_id):\n",
    "    fab_path = getFabsPDBID(pdb_id)\n",
    "    contacts = {}\n",
    "    for heavy_path in fab_path[\"heavy\"]:\n",
    "        for light_path in fab_path[\"light\"]:\n",
    "            n = getPairInterface(path_light = light_path, path_heavy = heavy_path)\n",
    "            if n > 3:\n",
    "                contacts[(light_path.name.rstrip(\".pkl\"),heavy_path.name.rstrip(\".pkl\"))] = n\n",
    "    return contacts\n",
    "\n",
    "fab_contacts = {}\n",
    "for pdb_id in getPDBList():\n",
    "    fab_contacts[pdb_id] = screenFabPairs(pdb_id)\n",
    "    print(pdb_id,fab_contacts[pdb_id])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(fab_contacts, open(\"./PDB_blast_db/fab_pairs.pkl\",'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Find antigens and interacting antibodies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllFabs():\n",
    "    return [l+\".pkl\" for l in fab_ids]\n",
    "\n",
    "def getAllAntigensList():\n",
    "    fab_ids = set(anarci_list_heavy)|set(anarci_list_light)\n",
    "    fab_ids = {f[:4].upper()+\"_\"+f[-1] for f in fab_ids}#print(fab_ids)\n",
    "    pdb_ids = [p for p in Path(\"./PDB_blast_db/structs_per_chain/\").glob(\"*.pkl\") if p.name[:6] not in fab_ids]\n",
    "    return pdb_ids\n",
    "\n",
    "def getAntigensPDBID(pdb_id = \"1LK3\"):\n",
    "    all_antigens = getAllAntigensList()\n",
    "    return [a for a in all_antigens if a.name[:4] == pdb_id]\n",
    "\n",
    "\n",
    "def screenAntigenContacts(pdb_id = \"1LK3\"):\n",
    "    if pdb_id not in fab_contacts:\n",
    "        return []\n",
    "    fab_pairs = fab_contacts[pdb_id]\n",
    "    antigens  = getAntigensPDBID(pdb_id)\n",
    "    hits = []\n",
    "    for antigen in antigens:\n",
    "        for fab_pair in fab_pairs:\n",
    "            fab_path_light = Path(\"./PDB_blast_db/structs_antibodies/\"+fab_pair[0]+\".pkl\")\n",
    "            fab_path_heavy = Path(\"./PDB_blast_db/structs_antibodies/\"+fab_pair[1]+\".pkl\")\n",
    "            n_light = testContact(antigen,fab_path_light)\n",
    "            n_heavy = testContact(antigen,fab_path_heavy)\n",
    "            if n_light+n_heavy == 0:\n",
    "                continue\n",
    "            hits.append([antigen,fab_pair,n_light,n_heavy])\n",
    "    return hits\n",
    "        \n",
    "def testContact(antigen_path, fab_path):\n",
    "    print(antigen_path)\n",
    "    antigen_df = pickle.load(antigen_path.open('rb'))\n",
    "    if antigen_df is None:\n",
    "        return 0\n",
    "\n",
    "    fab_df     = pickle.load(fab_path.open('rb'))\n",
    "    fab_type   = None\n",
    "    interface  = None\n",
    "    \n",
    "    if fab_path.name.split(\"_\")[-1] == \"light.pkl\":\n",
    "        fab_type  = \"light\"\n",
    "        interface = [\"L\"+str(i) for i in list(range(23,35))+list(range(66,72))+list(range(89,98))]\n",
    "        \n",
    "    elif fab_path.name.split(\"_\")[-1] == \"heavy.pkl\":\n",
    "        fab_type  = \"heavy\" \n",
    "        interface = [\"H\"+str(i) for i in list(range(23,35))+list(range(51,57))+list(range(93,102))]\n",
    "    \n",
    "    fab_interface = fab_df[fab_df[\"anarci\"].isin(interface)]\n",
    "    xyz_fab       = getxyz(fab_interface)\n",
    "    xyz_antigen   = getxyz(antigen_df)                   \n",
    "    \n",
    "    cd  = distance.cdist(xyz_antigen,xyz_fab)\n",
    "    ids = set(np.where(cd<4.5)[0])\n",
    "    \n",
    "    return len(ids)\n",
    "\n",
    "    \n",
    "hits = []\n",
    "for pdb_id in set([n.name[:4] for n in getAllAntigensList()]):\n",
    "    hits+=screenAntigenContacts(pdb_id)\n",
    "    \n",
    "fo = open(\"./PDB_blast_db/antigen_fabs.txt\",'w')\n",
    "for r in hits:\n",
    "    fo.write(f\"{r[0]} {r[1][0]} {r[1][1]} {r[2]} {r[3]}\\n\")\n",
    "fo.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. We have prepared dataframes and PDB structures including heavy / light fab pairs interacting with antigen.\n",
    "### Contact numbers are going to be calculated with PyMOL script contact_number.py <br>\n",
    "Here is an example for R1 = 4.5 and R2 = 12.0 <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R1 = 4.5\n",
    "R2 = 12.0 \n",
    "for fab_antigen_id in fab_antigen_ids:\n",
    "    cmd.do(f\"pymol -r contact_number.py {fab_antigen_id} {R1} {R2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Cluster sequences , extract release dates , and combine all contact numbers to single antigen-epitope dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful scripts for sequence clustering and dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from   pathlib import Path\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def alignmentLabels(ref_seq,tar_seq):\n",
    "    \"\"\"\n",
    "    Function to align residue position from two prealigned sequences ref_seq and tar_seq\n",
    "    \"\"\"\n",
    "    n  = [None for i in range(len(ref_seq.replace(\"-\",\"\")))]\n",
    "    n1 = 0\n",
    "    n2 = 0\n",
    "    for s1,s2 in zip(ref_seq,tar_seq):\n",
    "        if s1!=\"-\" and s2!=\"-\":\n",
    "            n[n1] = n2\n",
    "        if s1!=\"-\":\n",
    "            n1+=1\n",
    "        if s2!=\"-\":\n",
    "            n2+=1\n",
    "    return n\n",
    "\n",
    "#    \n",
    "\n",
    "def loadMSA(path,dataframe):\n",
    "    \"\"\"\n",
    "    Function to parse MSA\n",
    "    \"\"\"\n",
    "    seqs = []\n",
    "    for line in path.open('r'):\n",
    "        if line[0]==\">\":\n",
    "            seqs.append([])\n",
    "        seqs[-1].append(line.rstrip())\n",
    "\n",
    "    sequences = []\n",
    "    titles    = []\n",
    "    for s in seqs:\n",
    "        title = s[0][1:]\n",
    "        if title not in dataframe:\n",
    "            continue\n",
    "        sequences.append(\"\".join(s[1:]))\n",
    "        titles.append(s[0][1:])\n",
    "\n",
    "    sequences_resi = []\n",
    "    seq_ids = {}\n",
    "    msa_matrix = np.zeros((len(sequences),len(sequences[0]))).astype(int)-1\n",
    "\n",
    "    for u,[sequence,title] in enumerate(zip(sequences,titles)):\n",
    "        seq_ids[title] = []\n",
    "        n = 0\n",
    "        for i in range(len(sequence)):\n",
    "            if sequence[i] == \"-\":\n",
    "                continue\n",
    "            msa_matrix[u,i] = n\n",
    "            seq_ids[title].append(i)\n",
    "            n+=1\n",
    "    return msa_matrix, sequences, titles, seq_ids\n",
    "    \n",
    "def calculateConsensus(data, job_id = \"_fab\"):\n",
    "    \"\"\"\n",
    "    precalculate MSA for 95% identity clusters of antigen sequences\n",
    "    mmseqs is used to for clustering\n",
    "    kalign for MSA\n",
    "    \"\"\"\n",
    "    if not os.path.exists(f\"./temp{job_id}/\"):\n",
    "        os.mkdir(f\"temp{job_id}/\")\n",
    "    \n",
    "    fo = open(f\"./temp{job_id}/all_sequences.fasta\",'w')\n",
    "    sequences = {}\n",
    "    for pdbid in data:\n",
    "        seq = \"\".join(data[pdbid][\"aa\"])\n",
    "        if len(seq) < 50:\n",
    "            continue\n",
    "        fo.write(f\">{pdbid}\\n{seq}\\n\")\n",
    "        sequences[pdbid] = seq\n",
    "        print(pdbid,seq)\n",
    "    fo.close()\n",
    "        \n",
    "    command = \"\"\"mmseqs easy-cluster ../all_sequences.fasta results temp --min-seq-id 0.95 -c 0.9 --cov-mode 0 -s 8 --cluster-mode 1\"\"\"\n",
    "    \n",
    "    if not os.path.exists(f\"./temp{job_id}/clusters_095\"):\n",
    "        os.mkdir(f\"./temp{job_id}/clusters_095\")\n",
    "    \n",
    "   \n",
    "    clusters = {}\n",
    "    with open(f\"./temp{job_id}/clusters_095/results_cluster.tsv\") as f:\n",
    "        for line in f:\n",
    "            r = line.rstrip().split()\n",
    "            clusters.setdefault(r[0],set())\n",
    "            clusters[r[0]].add(r[1])\n",
    "    \n",
    "    if not os.path.exists(f\"./temp{job_id}/clusters_095/msa/\"):\n",
    "        os.mkdir(f\"./temp{job_id}/clusters_095/msa\")\n",
    "\n",
    "    fo_jobs = open(f\"./temp{job_id}/msa_095.sh\",'w')    \n",
    "    for cluster_ref in clusters:    \n",
    "        fo = open(f\"./temp{job_id}/clusters_095/msa/{cluster_ref}.fasta\",'w')\n",
    "        fo.write(f\">{cluster_ref}\\n{sequences[cluster_ref]}\\n\")\n",
    "        for c in clusters[cluster_ref]:\n",
    "            if c == cluster_ref:\n",
    "                continue\n",
    "            fo.write(f\">{c}\\n{sequences[c]}\\n\")\n",
    "        fo.close()\n",
    "        fo_jobs.write(f\"kalign -i ./clusters_095/msa/{cluster_ref}.fasta -o ./clusters_095/msa/{cluster_ref}_al.fasta\\n\")\n",
    "    fo_jobs.close()\n",
    "\n",
    "def consensusByMsa(dataframe,output_folder):\n",
    "    \"\"\"\n",
    "    parse precalculate MSA and calculate consensus contact numbers\n",
    "    \"\"\"\n",
    "    msa_path = Path(\"./temp_fab/clusters_095/msa/\").glob(\"*_al.fasta\")\n",
    "    for m in msa_path:\n",
    "        consensusByMsa_(dataframe,\n",
    "                        msa_path = str(m),\n",
    "                        output_path = output_folder)\n",
    "    \n",
    "def consensusByMsa_(dataframe,\n",
    "                    msa_path = \"./temp_fab/clusters_095/msa/7T0R_A_LH_al.fasta\",\n",
    "                    output_path = \"./temp_fab/clusters_095/pkls/\"):\n",
    "    \"\"\"\n",
    "    consensus contact numbers are calculated for antigens within 95% identity clusters \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    output_full_path = output_path+\"/\"+msa_path.split(\"/\")[-1]+\".pkl\"\n",
    "\n",
    "    if os.path.exists(output_full_path):\n",
    "        return\n",
    "    \n",
    "    msa, msa_seq, msa_name, msa_ids = loadMSA(Path(msa_path),dataframe)\n",
    "    \n",
    "    values = {\"cn_CA\":{},\"cn_raw\":{},\"cn_non_raw\":{}}\n",
    "    for seq_id in range(len(msa_name)):\n",
    "        for msa_id in range(msa.shape[1]):\n",
    "            resi_id = msa[seq_id,msa_id]\n",
    "            if resi_id == -1:\n",
    "                continue\n",
    "            aa = msa_seq[seq_id][msa_id]\n",
    "            for c in [\"cn_CA\",\"cn_raw\",\"cn_non_raw\"]:\n",
    "                values[c].setdefault(msa_id,{})\n",
    "                values[c][msa_id].setdefault(aa,[])\n",
    "                contacts = dataframe[msa_name[seq_id]][c].iloc[resi_id]\n",
    "                values[c][msa_id][aa].append(contacts)\n",
    "                \n",
    "    for contact_id in values:\n",
    "        for msa_id in values[contact_id]:\n",
    "            for aa in values[contact_id][msa_id]:\n",
    "                values[contact_id][msa_id][aa] = max(values[contact_id][msa_id][aa])\n",
    "\n",
    "    cluster = {}\n",
    "    for pdbid in msa_ids:\n",
    "        consensus = {\"cn_CA\":[],\"cn_raw\":[],\"cn_non_raw\":[]}\n",
    "        for resi_id,msa_id in enumerate(msa_ids[pdbid]):\n",
    "            aa = dataframe[pdbid].iloc[resi_id][\"aa\"]\n",
    "            assert aa in values[c][msa_id]\n",
    "            for c in values:\n",
    "                consensus[c].append(values[c][msa_id][aa])\n",
    "                \n",
    "        for c in consensus:\n",
    "            dataframe[pdbid][c+\"_consensus\"] = consensus[c]\n",
    "            \n",
    "        cluster[pdbid] = dataframe[pdbid]\n",
    "\n",
    "    if not os.path.exists(output_path):\n",
    "        os.mkdir(output_path)\n",
    "    \n",
    "    pickle.dump(cluster,open(output_full_path,'wb'))\n",
    "\n",
    "    return        \n",
    "        \n",
    "\n",
    "def combineDataframes(input_ = \"./structs_antigen_fab_df/\" ,\n",
    "                      output_ = \"./true_antigen_fab_cn_12.0_6.0.pkl\",\n",
    "                      R = \"12.0_6.0\"):\n",
    "    \"\"\"\n",
    "    combine dataframes\n",
    "    \"\"\"\n",
    "    r = Path(input_).glob(f\"*_{R}.pkl\")\n",
    "    contact_numbers = {}    \n",
    "    for r_ in r:\n",
    "        df = pickle.load(r_.open('rb'))\n",
    "        df = pd.DataFrame(df)\n",
    "        df[\"name\"] = r_.name.rstrip(\".pkl\")\n",
    "        pdb_name = r_.name.split(\"_\")[:3]\n",
    "        pdb_name = \"_\".join(pdb_name)\n",
    "        contact_numbers[pdb_name] = df\n",
    "    pickle.dump(contact_numbers, open(output_,'wb'))\n",
    "\n",
    "    \n",
    "def verySimilarClusters(data, job_id = \"_all\"):\n",
    "    \"\"\"\n",
    "    cluster sequences by 95% identity\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(f\"./temp{job_id}/\"):\n",
    "        os.mkdir(f\"./temp{job_id}/\")\n",
    "        \n",
    "    if not os.path.exists(f\"./temp{job_id}/clusters_095\"):\n",
    "        os.mkdir(f\"./temp{job_id}/clusters_095\")\n",
    "        \n",
    "    fo = open(f\"./temp{job_id}/clusters_095/all_sequences.fasta\",'w')\n",
    "    sequences = {}\n",
    "    for pdbid in data:\n",
    "        seq = \"\".join(data[pdbid][\"aa\"])\n",
    "        if len(seq) < 50:\n",
    "            continue\n",
    "        fo.write(f\">{pdbid}\\n{seq}\\n\")\n",
    "        sequences[pdbid] = seq\n",
    "        print(pdbid,seq)\n",
    "    fo.close()\n",
    "    \n",
    "    command = \"\"\"mmseqs easy-cluster all_sequences.fasta results temp --min-seq-id 0.95 -c 0.9 --cov-mode 0 -s 8 --cluster-mode 1\"\"\"\n",
    "\n",
    "    subprocess.call(command,shell=True,cwd = f\"./temp{job_id}/clusters_095/\")\n",
    "\n",
    "    clusters = {}\n",
    "    with open(f\"./temp{job_id}/clusters_095/results_cluster.tsv\") as f:\n",
    "        for line in f:\n",
    "            r = line.rstrip().split()\n",
    "            clusters.setdefault(r[0],set())\n",
    "            clusters[r[0]].add(r[1])\n",
    "\n",
    "    clu_id = 0\n",
    "    clusters_ = {}\n",
    "    for c in clusters:\n",
    "        for pdb_id in clusters[c]:\n",
    "            clusters_[pdb_id] = clu_id\n",
    "        clu_id +=1\n",
    "\n",
    "    for pdb_id in sequences:\n",
    "        print(pdb_id)\n",
    "        assert pdb_id in clusters_\n",
    "        data[pdb_id][\"very_similar_cluster_id\"] = clusters_[pdb_id]\n",
    "    \n",
    "    return clusters_\n",
    "\n",
    "def notverySimilarClusters(data, job_id = \"_all\"):\n",
    "    \"\"\"\n",
    "    cluster sequences by 70% identity\n",
    "    \"\"\"\n",
    "    if not os.path.exists(f\"./temp{job_id}/\"):\n",
    "        os.mkdir(f\"./temp{job_id}/\")\n",
    "        \n",
    "    if not os.path.exists(f\"./temp{job_id}/clusters_07\"):\n",
    "        os.mkdir(f\"./temp{job_id}/clusters_07\")\n",
    "\n",
    "    fo = open(f\"./temp{job_id}/clusters_07/all_sequences.fasta\",'w')\n",
    "    sequences = {}\n",
    "    for pdbid in data:\n",
    "        seq = \"\".join(data[pdbid][\"aa\"])\n",
    "        if len(seq) < 50:\n",
    "            continue\n",
    "        fo.write(f\">{pdbid}\\n{seq}\\n\")\n",
    "        sequences[pdbid] = seq\n",
    "        print(pdbid,seq)\n",
    "    fo.close()\n",
    "\n",
    "    command = f\"\"\"mmseqs easy-cluster all_sequences.fasta results temp --min-seq-id 0.7 -c 0.7 --cov-mode 1 -s 8 --cluster-mode 1\"\"\"\n",
    "\n",
    "    subprocess.call(command,shell=True,cwd = f\"./temp{job_id}/clusters_07/\")\n",
    "\n",
    "    clusters = {}\n",
    "    with open(f\"./temp{job_id}/clusters_07/results_cluster.tsv\") as f:\n",
    "        for line in f:\n",
    "            r = line.rstrip().split()\n",
    "            clusters.setdefault(r[0],set())\n",
    "            clusters[r[0]].add(r[1])\n",
    "\n",
    "    clu_id = 0\n",
    "    clusters_ = {}\n",
    "    for c in clusters:\n",
    "        for pdb_id in clusters[c]:\n",
    "            clusters_[pdb_id] = clu_id\n",
    "        clu_id +=1\n",
    "\n",
    "    for pdb_id in sequences:\n",
    "        print(pdb_id)\n",
    "        assert pdb_id in clusters_\n",
    "        data[pdb_id][\"notvery_similar_cluster_id\"] = clusters_[pdb_id]\n",
    "    \n",
    "    return clusters_\n",
    " \n",
    "\n",
    "\n",
    "def combineAllData(exp_type = \"_12.0_4.5\"):\n",
    "    if not os.path.exists(f\"all_antigen_noclusters{exp_type}.pkl\"):\n",
    "        df_fab = {}\n",
    "        df_msa_path = Path(f\"./msa{exp_type}/\").glob(\"*.pkl\")\n",
    "        clu_id = 0\n",
    "        \n",
    "        for d in df_msa_path:\n",
    "            dfs = pickle.load(d.open('rb'))\n",
    "            \n",
    "            for pdb_id in dfs:\n",
    "                dfs[pdb_id][\"cluster_id\"] = clu_id\n",
    "            df_fab.update(dfs)\n",
    "\n",
    "            clu_id+=1\n",
    "            print(clu_id,len(df_fab))\n",
    "         \n",
    "        df1  = pickle.load(open(f\"./true_antigen_fab_cn{exp_type}.pkl\",'rb'))\n",
    "        \n",
    "        print(len(df_fab),\"some missing?\")\n",
    "        print(len(df_fab),\"before\")\n",
    "        \n",
    "        for pdb_id in df1:\n",
    "            if df1[pdb_id].shape[0]<50:\n",
    "                continue\n",
    "            if pdb_id not in df_fab:\n",
    "                df_fab[pdb_id] = df1[pdb_id]\n",
    "\n",
    "        print(len(df_fab),\"after\")\n",
    "        print(\"now add missing fake antigens\")\n",
    "         \n",
    "        pickle.dump(df_fab,open(f\"all_antigen_noclusters{exp_type}.pkl\",'wb'))\n",
    "    else:\n",
    "        df_all = pickle.load(open(f\"all_antigen_noclusters{exp_type}.pkl\",'rb'))\n",
    "\n",
    "    print(\"now cluster very similar\")\n",
    "    verySimilarClusters(df_all,job_id = exp_type)\n",
    "\n",
    "    print(\"now cluster not very similar\")\n",
    "    notverySimilarClusters( df_all, job_id = exp_type)\n",
    "\n",
    "    pickle.dump(df_all, open(f\"all_antigen_data{exp_type}.pkl\",'wb'))\n",
    "    \n",
    "        \n",
    "def putDates():\n",
    "    \"\"\"\n",
    "    pdb_release_data.txt file is download from rcsb.org \n",
    "    function add release dates column for each antigen sequence cluster\n",
    "    \"\"\"\n",
    "    date = {}\n",
    "    with open(\"pdb_release_date.txt\") as f:\n",
    "        for line in f:\n",
    "            r = line.rstrip().split()\n",
    "            \n",
    "            d = int(r[1].split(\"-\")[-1])\n",
    "            if d>=23:\n",
    "                d = 1900+d\n",
    "            else:\n",
    "                d = 2000+d\n",
    "            date[r[0]] = d\n",
    "\n",
    "    data = pickle.load(open(\"all_antigen_data.pkl\",'rb'))\n",
    "    \n",
    "    cluster_antigen_release_dates    = {}\n",
    "    cluster_notantigen_release_dates = {}\n",
    "\n",
    "    for pdb_id in data:\n",
    "        if len(pdb_id)!=9:\n",
    "            continue\n",
    "        pdb4 = pdb_id.split(\"_\")[0].upper()\n",
    "        cluster_id = data[pdb_id][\"notvery_similar_cluster_id\"][0]\n",
    "        cluster_antigen_release_dates.setdefault(cluster_id, [])\n",
    "        cluster_antigen_release_dates[cluster_id].append(date[pdb4])\n",
    "        \n",
    "    for c in cluster_antigen_release_dates:\n",
    "        cluster_antigen_release_dates[c] = min(cluster_antigen_release_dates[c])\n",
    "\n",
    "    print(cluster_antigen_release_dates)\n",
    "    cluster_notantigen_release_dates = {}\n",
    "    for pdb_id in data:\n",
    "        if len(pdb_id)!=8:\n",
    "            continue\n",
    "        pdb4 = pdb_id.split(\"_\")[0].upper()\n",
    "        cluster_id = data[pdb_id][\"notvery_similar_cluster_id\"][0]\n",
    "        cluster_notantigen_release_dates.setdefault(cluster_id, [])\n",
    "        cluster_notantigen_release_dates[cluster_id].append(date[pdb4])\n",
    "\n",
    "    print(cluster_notantigen_release_dates)\n",
    "\n",
    "    for c in cluster_notantigen_release_dates:\n",
    "        cluster_notantigen_release_dates[c] = min(cluster_notantigen_release_dates[c])\n",
    "    \n",
    "        \n",
    "    for pdb_id in data:\n",
    "        pdb4 = pdb_id.split(\"_\")[0].upper()\n",
    "        cluster_id = data[pdb_id][\"notvery_similar_cluster_id\"][0]\n",
    "        if len(pdb_id) == 9:\n",
    "            cluster_release_date = cluster_antigen_release_dates[cluster_id] \n",
    "        else:\n",
    "            cluster_release_date = cluster_notantigen_release_dates[cluster_id] \n",
    "        data[pdb_id][\"release_date\"] = date[pdb4]\n",
    "        data[pdb_id][\"cluster_release_date\"] = cluster_release_date\n",
    "\n",
    "    pickle.dump(data, open(\"all_antigen_data_date.pkl\",'wb'))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(R1 = 6.0, R2 = 12.0):\n",
    "    ### first combine all calculated contact numbers by PyMOL\n",
    "    epitope_labels = f\"./eptiope_labels_cn_{R1}_{R2}.pkl\"\n",
    "    \n",
    "    if not os.path.exists(epitope_labels):\n",
    "        combineDataframes(input_ = \"./structs_antigen_fab_df/\",\n",
    "                          output_ = epitope_labels,\n",
    "                          R = f\"{R1}_{R2}\")\n",
    "    df1  = pickle.load(open(epitope_labels,'rb'))\n",
    "        \n",
    "    ### cluster sequences by similarity and calculate MSA for each cluster\n",
    "    calculateConsensus(df1)\n",
    "    \n",
    "    ### get consensus contact number values according to MSA\n",
    "    consensusByMsa(df1,\n",
    "                   output_folder = f\"./msa_{R1}_{R2}/\")\n",
    "\n",
    "    ### combine all data to single dataframe\n",
    "    all_data_path = f\"all_antigen_data_{R1}_{R2}.pkl\"\n",
    "    if not os.path.exists(all_data_path):\n",
    "        combineAllData(f\"_{R1}_{R2}\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example to generate dataset for R1 = 4.5 and R2 = 16.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline(R1 = 4.5, R2 = 16.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

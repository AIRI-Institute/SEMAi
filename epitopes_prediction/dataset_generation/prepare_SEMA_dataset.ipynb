{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Fetch PDB sequences and generate blast DB <br>\n",
    "\n",
    "Each PDB formatted file includes \"SEQRES records\" which list the primary sequence of the polymeric molecules present in the entry. This sequence information is also available as a FASTA download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from biopandas.pdb import PandasPdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from Bio.Blast import NCBIXML\n",
    "from pathlib import Path\n",
    "import sys \n",
    "\n",
    "if not os.path.exists(\"PDB_blast_db\"):\n",
    "    os.mkdir(\"PDB_blast_db\")\n",
    "\n",
    "if not os.path.exists(\"PDB_blast_db/pdb_seqres.txt\"):\n",
    "    subprocess.call(\"wget https://files.wwpdb.org/pub/pdb/derived_data/pdb_seqres.txt.gz\",shell=True)    \n",
    "    subprocess.call(\"gzip -d pdb_seqres.txt.gz\",shell=True)\n",
    "    subprocess.call(\"mv pdb_seqres.txt ./PDB_blast_db/\",shell=True)\n",
    "    \n",
    "if not os.path.exists(\"PDB_blast_db/pdb_seqres.txt.psq\"):\n",
    "    subprocess.call(\"makeblastdb -in pdb_seqres.txt -dbtype prot -title pdb\",shell=True, cwd=\"./PDB_blast_db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preliminary screen for proteins in the PDB database with homology to fragment antigen-binding region "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch PDB's with Fab's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "light = \"DILLTQSPVILSVSPGERVSFSCRASQSIGTNIHWYQQRTNGSPRLLIKYASESISGIPSRFSGSGSGTDFTLSINSVESEDIADYYCQQNNNWPTTFGAGTKLELK\"\n",
    "with open(\"PDB_blast_db/fab_light.fasta\",'w') as fo:\n",
    "    fo.write(\">input_light\\n\")\n",
    "    fo.write(light)\n",
    "\n",
    "heavy = \"QVQLKQSGPGLVQPSQSLSITCTVSGFSLTNYGVHWVRQSPGKGLEWLGVIWSGGNTDYNTPFTSRLSINKDNSKSQVFFKMNSLQSNDTAIYYCARALTYYDYEFAYWGQGTLVTVSA\"\n",
    "with open(\"PDB_blast_db/fab_heavy.fasta\",'w') as fo:\n",
    "    fo.write(\">input_heavy\\n\")\n",
    "    fo.write(heavy)\n",
    "\n",
    "if not os.path.exists(\"PDB_blast_db/hits_fabs_light.xml\"):\n",
    "    subprocess.call(\"blastp -db pdb_seqres.txt -num_alignments 99999 -evalue 1e-9 -query fab_light.fasta -out hits_fabs_light.xml -outfmt 5\",shell=True, cwd=\"./PDB_blast_db\")\n",
    "\n",
    "if not os.path.exists(\"PDB_blast_db/hits_fabs_heavy.xml\"):\n",
    "    subprocess.call(\"blastp -db pdb_seqres.txt -num_alignments 99999 -evalue 1e-9 -query fab_heavy.fasta -out hits_fabs_heavy.xml -outfmt 5\",shell=True, cwd=\"./PDB_blast_db\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_blast_output(input_path): \n",
    "    result=open(input_path,\"r\")\n",
    "    records= NCBIXML.parse(result)\n",
    "    item=next(records)\n",
    "    pdb_fabs  = set()\n",
    "    pdb_fabs_ = set()\n",
    "    for alignment in item.alignments:\n",
    "        #print(alignment)\n",
    "        #break\n",
    "        for hsp in alignment.hsps:\n",
    "            pdb_id = alignment.title.split()[1]\n",
    "            pdb_id_id = pdb_id.split(\"_\")[0]\n",
    "            pdb_fabs.add(pdb_id)\n",
    "            pdb_fabs_.add(pdb_id_id)\n",
    "    return pdb_fabs\n",
    "\n",
    "pdb_fab_hits_1 =  parse_blast_output(\"PDB_blast_db/hits_fabs_light.xml\")\n",
    "pdb_fab_hits_2 =  parse_blast_output(\"PDB_blast_db/hits_fabs_heavy.xml\")\n",
    "pdb_fab_hits   = pdb_fab_hits_1|pdb_fab_hits_2\n",
    "\n",
    "print(len(pdb_fab_hits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Screen for heavy and light fab chains using ANARCI\n",
    "\n",
    "http://opig.stats.ox.ac.uk/webapps/newsabdab/sabpred/anarci/ <br>\n",
    "https://github.com/oxpig/ANARCI\n",
    "\n",
    "Annotate all Fabs with ANARCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fasta(path):\n",
    "    r = []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            if line[0]==\">\":\n",
    "                r.append([])\n",
    "            r[-1].append(line.rstrip())\n",
    "    r = [[r_[0],\"\".join(r_[1:])] for r_ in r]\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = []\n",
    "with open(\"./PDB_blast_db/pdb_seqres.txt\") as f:\n",
    "    for line in f:\n",
    "        if line[0]==\">\":\n",
    "            r.append([])\n",
    "        r[-1].append(line)\n",
    "        \n",
    "pdb_seqres_fasta = load_fasta(\"./PDB_blast_db/pdb_seqres.txt\")\n",
    "\n",
    "rfabs = []\n",
    "for r_ in r:\n",
    "    title = r_[0].split(\" \")[0][1:]\n",
    "    if title not in pdb_fab_hits:\n",
    "        continue\n",
    "    rfabs.append([r_[0].split(\" \")[0][1:],r_[1]])\n",
    "    \n",
    "with open(\"./PDB_blast_db/putative_fabs.fasta\",'w') as fo:\n",
    "    for r in rfabs:\n",
    "        fo.write(\"\".join([\">\"+r[0]+\"\\n\",r[1]])+\"\\n\")\n",
    "\n",
    "if not os.path.exists(\"./PDB_blast_db/all_fabs_heavy.anarci\"):\n",
    "    anarci_command = \"ANARCI -i putative_fabs.fasta -o all_fabs_heavy.anarci -s chothia -r ig --ncpu 8 --bit_score_threshold 100 --restrict heavy\"\n",
    "    subprocess.call(anarci_command,shell=True,cwd=\"./PDB_blast_db\")\n",
    "    \n",
    "if not os.path.exists(\"./PDB_blast_db/all_fabs_light.anarci\"):\n",
    "    anarci_command = \"ANARCI -i putative_fabs.fasta -o all_fabs_light.anarci -s chothia -r ig --ncpu 8 --bit_score_threshold 100 --restrict light\"\n",
    "    subprocess.call(anarci_command,shell=True,cwd=\"./PDB_blast_db\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Parse ANARCI output and extract heavy and light Fab sequences \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_anarci_annotation(path = \"light.anarci\",n=108):\n",
    "    seqs = []\n",
    "    seqs.append([[] for i in range(n)])\n",
    "    used = set()\n",
    "    data = {}\n",
    "    with open(path) as f:\n",
    "        w = f.readlines()\n",
    "        data = [[]]\n",
    "        for u,line in enumerate(w):\n",
    "            data[-1].append(line)\n",
    "            if line[0]==\"/\":\n",
    "                data.append([])          \n",
    "        out = {}\n",
    "        for d in data:\n",
    "            if len(d)==0:\n",
    "                continue\n",
    "            name = d[0].rstrip().split()[-1]\n",
    "            if name in out:\n",
    "                continue\n",
    "            out[name] = [[] for i in range(n)]            \n",
    "            for d_ in d:\n",
    "                if d_[0]==\"#\":\n",
    "                    continue\n",
    "                if d_[0]==\"/\":\n",
    "                    continue\n",
    "                id_ = d_.split()[1]\n",
    "                id_ = int(id_)\n",
    "                if d_[10]==\"-\":\n",
    "                    continue\n",
    "                out[name][id_].append(d_[10])\n",
    "    out_ = {}\n",
    "    for name in out:\n",
    "        if len(\"\".join([\"\".join(c) for c in out[name]]))==0:\n",
    "            continue\n",
    "        out_[name] = out[name]\n",
    "    return out_\n",
    "\n",
    "anarci_list_heavy = parse_anarci_annotation(\"./PDB_blast_db/all_fabs_heavy.anarci\", n=120)\n",
    "anarci_list_light = parse_anarci_annotation(\"./PDB_blast_db/all_fabs_light.anarci\", n=108)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Fetch all PDB structures containing Light/Heavy chains "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pdb_3 = {r[:4]:{\"light\":[],\"heavy\":[]} for r in list(pdb_fab_hits)}\n",
    "for h in anarci_list_light:\n",
    "    h4 = h[:4]\n",
    "    if h4 not in pdb_3:\n",
    "        continue\n",
    "    pdb_3[h4][\"light\"].append(h)\n",
    "for h in anarci_list_heavy:\n",
    "    h4 = h[:4]\n",
    "    if h4 not in pdb_3:\n",
    "        continue\n",
    "    pdb_3[h4][\"heavy\"].append(h)\n",
    "    \n",
    "if not os.path.exists(\"PDB_blast_db/structs\"):\n",
    "    os.mkdir(\"PDB_blast_db/structs\")\n",
    "\n",
    "for pdb_ in pdb_3:\n",
    "    if len(pdb_3[pdb_][\"light\"])+len(pdb_3[pdb_][\"heavy\"])==0:\n",
    "        continue    \n",
    "    pdb_name = pdb_.upper()+\".pdb.gz\"\n",
    "    if os.path.exists(\"PDB_blast_db/structs/\"+pdb_name):\n",
    "        continue\n",
    "    if os.path.exists(\"PDB_blast_db/structs/\"+pdb_name.rstrip(\".gz\")):\n",
    "        continue\n",
    "    subprocess.call(f\"wget https://files.rcsb.org/download/{pdb_name}\",shell=True, cwd=\"./PDB_blast_db/structs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test that PDB IDS corresponding to old ids are not missing\n",
    "train = pd.read_csv(\"../data/sema_1.0/train_set.csv\")\n",
    "test = pd.read_csv(\"../data/sema_1.0/test_set.csv\")\n",
    "ref_names = [d[\"pdb_id_chain\"][:6] for d in train.iloc()]\n",
    "ref_names+= [d[\"pdb_id_chain\"][:6] for d in test.iloc()]\n",
    "ref_names = set(ref_names)\n",
    "ref_pdbs  = set([r.split(\"_\")[0] for r in ref_names])\n",
    "\n",
    "downloaded_pdbs = set()\n",
    "for p in Path(\"./PDB_blast_db/structs/\").glob(\"*.pdb\"):\n",
    "    downloaded_pdbs.add(p.name[:4])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep First model for PDBs with multiple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "subprocess.call(f\"gzip -d *.gz\", shell=True, cwd=\"./PDB_blast_db/structs\")\n",
    "pdbs = Path(\"./PDB_blast_db/structs/\").glob(\"*.pdb\")\n",
    "for pdb in pdbs:\n",
    "    pdb_data = pdb.open('r').readlines()\n",
    "    fo = open(str(pdb),'w')\n",
    "    for l in pdb_data:\n",
    "        fo.write(l)\n",
    "        if l.startswith(\"ENDMDL\"):\n",
    "            break\n",
    "    fo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Prepare PDB dataframes and align full sequence (from pdb seq-res) on sequence of resloved protein (may contain some gaps) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aa_3_to_1(resn):\n",
    "    #assert line[:4] in {\"HETA\",\"ATOM\"}\n",
    "    #resn = line[17:20]\n",
    "    d = {'CYS': 'C', 'ASP': 'D', 'SER': 'S', 'GLN': 'Q', 'LYS': 'K',\n",
    "     'ILE': 'I', 'PRO': 'P', 'THR': 'T', 'PHE': 'F', 'ASN': 'N',\n",
    "     'GLY': 'G', 'HIS': 'H', 'LEU': 'L', 'ARG': 'R', 'TRP': 'W',\n",
    "     'ALA': 'A', 'VAL':'V', 'GLU': 'E', 'TYR': 'Y', 'MET': 'M'}\n",
    "    return d[resn]\n",
    "\n",
    "def kalign(seq1,seq2):\n",
    "    if not os.path.exists(\"./PDB_blast_db/temp\"):\n",
    "        os.mkdir(\"./PDB_blast_db/temp\")\n",
    "    fo = open(\"./PDB_blast_db/temp/input.fasta\",'w')\n",
    "    fo.write(f\">1\\n{seq1}\\n>2\\n{seq2}\\n\")\n",
    "    fo.close()\n",
    "    d = subprocess.check_output(\"cat ./PDB_blast_db/temp/input.fasta | kalign -f fasta\",shell=True)\n",
    "    res_  = d.decode(\"UTF-8\").split(\"\\n\")\n",
    "    res   = []\n",
    "    #print(res_)\n",
    "    for l in res_:\n",
    "        if len(l) ==0:\n",
    "            continue\n",
    "        if l[0]==\">\":\n",
    "            res.append([])\n",
    "        if len(res)!=0:\n",
    "            res[-1].append(l.rstrip(\"\\n\"))\n",
    "    #print(res_)\n",
    "    return \"\".join(res[0][1:]),\"\".join(res[1][1:])\n",
    "    \n",
    "    \n",
    "def remove_alternative_conformations(pdb_dataframe):\n",
    "    return pdb_dataframe[(pdb_dataframe[\"alt_loc\"] == \"A\") | (pdb_dataframe[\"alt_loc\"] == \" \")  | (pdb_dataframe[\"alt_loc\"] == \"\")]\n",
    "    \n",
    "def remove_unk(pdb_dataframe):\n",
    "    r = pdb_dataframe[\"residue_name\"] == \"UNK\"\n",
    "    return pdb_dataframe[~r]#( | (pdb_dataframe[\"alt_loc\"] == \" \")  | (pdb_dataframe[\"alt_loc\"] == \"\")]\n",
    "    \n",
    "def consider_insertions(pdb_dataframe):\n",
    "    r1 = pdb_dataframe[\"residue_number\"]\n",
    "    r2 = pdb_dataframe[\"insertion\"]\n",
    "    r3 = pdb_dataframe[\"chain_id\"]\n",
    "    r4 = pdb_dataframe[\"residue_name\"]\n",
    "    ra = [(r_1,r_2,r_3,r_4) for (r_1,r_2,r_3,r_4) in zip(r1,r2,r3,r4)]\n",
    "    pdb_dataframe[\"residue_key\"] = ra\n",
    "    return pdb_dataframe\n",
    "    \n",
    "def put_full_sequence(pdb_dataframe, full_seq):\n",
    "    pdb_dataframe = remove_alternative_conformations(pdb_dataframe)\n",
    "    pdb_dataframe = remove_unk(pdb_dataframe)\n",
    "    pdb_dataframe = consider_insertions(pdb_dataframe)\n",
    "    \n",
    "    if len(pdb_dataframe) == 0:\n",
    "        print(\"Empty...\")\n",
    "        return\n",
    "    #print(pdb_dataframe)\n",
    "    #assert 1==2\n",
    "    pdb_ca                          = pdb_dataframe[pdb_dataframe[\"atom_name\"] == \"CA\"]\n",
    "    \n",
    "    residue_numbers = []\n",
    "    residue_seq     = []\n",
    "    used = set()\n",
    "    \n",
    "    #### To avoid residue duplicates \n",
    "    #### Might be not necessary\n",
    "    for r in pdb_ca.iloc():#[\"residue_number\"]:\n",
    "        residue_number = r[\"residue_key\"]\n",
    "        if residue_number in used:\n",
    "            continue\n",
    "        residue_numbers.append(residue_number)\n",
    "        residue_seq.append(    aa_3_to_1(r[\"residue_name\"]))\n",
    "    \n",
    "    pdb_seq = \"\".join(residue_seq)#pdb_seq)    \n",
    "    if len(pdb_seq) <= 5:\n",
    "        print(\"PDB sequence is too short\")\n",
    "        return\n",
    "    pdb_seq_aligned, full_seq_aligned = kalign(pdb_seq,full_seq)\n",
    "\n",
    "    assert full_seq_aligned.replace(\"-\",\"\") == full_seq\n",
    "    \n",
    "    n_pdb     = -1\n",
    "    n_pdb_map = []\n",
    "    \n",
    "    print(\"pdb\",pdb_seq_aligned)\n",
    "    print(\"full\",full_seq_aligned)\n",
    "    \n",
    "    assert len(pdb_seq_aligned) == len(full_seq_aligned)\n",
    "    \n",
    "    new_dataframe = []\n",
    "    for i,[a_pdb,a_fullseq] in enumerate(zip(list(pdb_seq_aligned),list(full_seq_aligned))):\n",
    "        #print(a_pdb,a_fullseq,residue_numbers[n_pdb])\n",
    "        if a_pdb != '-':\n",
    "            n_pdb+=1\n",
    "        #assert a_fullseq != \"-\"\n",
    "        #if a_fullseq!=\"-\" and a_pdb!=\"-\":\n",
    "        #    assert a_fullseq == a_pdb\n",
    "        if a_pdb == \"-\":\n",
    "            n_pdb_map.append({\"resi\":None,                  \"a_pdb\":None, \"a_full\":a_fullseq})\n",
    "        else:\n",
    "            n_pdb_map.append({\"resi\":residue_numbers[n_pdb],\"a_pdb\":a_pdb,\"a_full\":a_fullseq})\n",
    "\n",
    "    full_df  = []\n",
    "    full_seq = []\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "    \n",
    "    for r in n_pdb_map:\n",
    "        full_seq.append(r[\"a_full\"])\n",
    "        \n",
    "        if r[\"resi\"] is None:\n",
    "            empty_   = pd.DataFrame(np.nan, index=[0],columns=pdb_ca.columns)\n",
    "            empty_[\"atom_name\"] = \"CA\"\n",
    "            empty_[\"seqres\"]    = r[\"a_full\"]\n",
    "            full_df.append(empty_)\n",
    "            continue\n",
    "        \n",
    "        pdb_residue = pdb_dataframe[pdb_dataframe[\"residue_key\"] == r[\"resi\"]]\n",
    "        pdb_residue[\"seqres\"] = r[\"a_full\"]\n",
    "        pdb_residue[\"aa\"]     = r[\"a_pdb\"]\n",
    "        full_df.append(pdb_residue)  \n",
    "    full_df = pd.concat(full_df,axis=0,ignore_index=True)\n",
    "    print(full_df.shape)\n",
    "    return full_df\n",
    "\n",
    "def get_tasks():\n",
    "    if not os.path.exists(\"./PDB_blast_db/all_pdbids_and_chains.txt\"):\n",
    "        all_chains = set()\n",
    "        p = Path(\"./PDB_blast_db/structs/\").glob(\"*.pdb\")    \n",
    "        for p_ in p:\n",
    "            for line in p_.open('r'):\n",
    "                if line.startswith(\"ATOM\") and line[13:15] ==\"CA\":\n",
    "                    all_chains.add(p_.name.rstrip(\".pdb\")+\"_\"+line[21])                \n",
    "        with open(\"./PDB_blast_db/all_pdbids_and_chains.txt\",'w') as fo:\n",
    "            fo.write(\"\\n\".join(list(all_chains)))\n",
    "\n",
    "    all_chains = [r.rstrip() for r in open(\"./PDB_blast_db/all_pdbids_and_chains.txt\",'r').readlines()]\n",
    "    used = set()\n",
    "    for p in Path(\"./PDB_blast_db/structs_per_chain/\").glob(\"*.pkl\"):\n",
    "        used.add(p.name.rstrip(\".pkl\"))\n",
    "    j = {}\n",
    "    for u in all_chains:\n",
    "        if u in used:\n",
    "            continue\n",
    "        pdbid,chain = u.split(\"_\")\n",
    "        j.setdefault(pdbid,set())\n",
    "        j[pdbid].add(chain)\n",
    "        \n",
    "    return j\n",
    "\n",
    "\n",
    "\n",
    "def get_PDBDataFrame(pdb_id = \"1FGV\",chains = None):   \n",
    "    pdb_path           = f\"./PDB_blast_db/structs/{pdb_id.upper()}.pdb\"\n",
    "    if not os.path.exists(pdb_path):\n",
    "        print(pdb_id,\" not found\")\n",
    "        return\n",
    "    pdb_structure      = PandasPdb().read_pdb(pdb_path).df[\"ATOM\"]\n",
    "    from Bio import SeqIO\n",
    "    \n",
    "    sequences = {}\n",
    "    pdb_records = {}\n",
    "    \n",
    "    if not os.path.exists(f\"./PDB_blast_db/structs_per_chain/\"):\n",
    "        os.mkdir(f\"./PDB_blast_db/structs_per_chain/\")\n",
    "        \n",
    "    for record in SeqIO.parse(pdb_path, \"pdb-seqres\"):\n",
    "        chain             = record.id[-1]            \n",
    "        if os.path.exists(f\"./PDB_blast_db/structs_per_chain/{pdb_id}_{chain}.pkl\"):\n",
    "            continue\n",
    "\n",
    "        if chains is not None and chain not in chains:\n",
    "            continue\n",
    "        sequences[chain]  = record.seq\n",
    "        pdb_chain         = pdb_structure[pdb_structure[\"chain_id\"] == chain]\n",
    "        if len(pdb_chain) == 0:\n",
    "            continue\n",
    "        \n",
    "        print(pdb_id,chain)\n",
    "        pdb_chain_fullseq = put_full_sequence(pdb_chain,sequences[chain])\n",
    "        pickle.dump(pdb_chain_fullseq, open(f\"./PDB_blast_db/structs_per_chain/{pdb_id}_{chain}.pkl\",'wb'))\n",
    "    \n",
    "#df_jobs = {}\n",
    "jobs = get_tasks()\n",
    "for pdb_id in jobs:#get_tasks():#pdb_3:\n",
    "    print(pdb_id)\n",
    "    chains = jobs[pdb_id]\n",
    "    get_PDBDataFrame(pdb_id,chains)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Put ANARCI annotation into antibodies dataframes prepared in the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mafft_align(s1,s2,strict=True):\n",
    "    with open(\"m.fasta\",'w') as fo:\n",
    "        fo.write(\">1\\n\"+s1+\"\\n>2\\n\"+s2+\"\\n\")\n",
    "    if not strict:\n",
    "        d = subprocess.check_output(\"mafft --anysymbol --op 0.1  m.fasta \",shell=True)\n",
    "    else:\n",
    "        d = subprocess.check_output(\"mafft --anysymbol --auto m.fasta \",shell=True)\n",
    "\n",
    "    res_  = d.decode(\"UTF-8\").split(\"\\n\")\n",
    "    res   = []\n",
    "    for l in res_:\n",
    "        if len(l)==0:\n",
    "            continue\n",
    "        if l[0]==\">\":\n",
    "            res.append(\"\")\n",
    "            continue\n",
    "        res[-1]+=l.rstrip()\n",
    "    return res\n",
    "\n",
    "def realign_sequences(pdb_seq,anarci_, firstLetterException = False):\n",
    "    seq_aa = []\n",
    "    seq_i  = []\n",
    "    for i,s_ in enumerate(anarci_):\n",
    "        if len(s_)==0:\n",
    "            continue\n",
    "        seq_aa+=s_\n",
    "        seq_i +=[i for i_ in range(len(s_))]\n",
    "    al = kalign(\"\".join(seq_aa),\"\".join(pdb_seq))\n",
    "    \n",
    "    seq_anarci_aligned = al[0]\n",
    "    pdb_seq_aligned    = al[1]\n",
    "    n_anarci = 0\n",
    "    n_pdb    = 0\n",
    "    pdb_anarci_map = [None for i in pdb_seq]\n",
    " \n",
    "    for i,[a_anarci,a_pdb] in enumerate(zip(*al)):\n",
    "        if a_anarci!=\"-\" and a_pdb!=\"-\":#i!=0:\n",
    "            pdb_anarci_map[n_pdb] = i\n",
    "            if n_anarci == 0 and firstLetterException:\n",
    "                n_pdb+=1\n",
    "                n_anarci+=1\n",
    "                continue\n",
    "            if a_pdb!=a_anarci:\n",
    "                return None\n",
    "        if a_pdb!=\"-\":\n",
    "            n_pdb+=1\n",
    "        if a_anarci!=\"-\":            \n",
    "            n_anarci+=1\n",
    "\n",
    "    return pdb_anarci_map\n",
    "    \n",
    "def put_anarci_annotation(pdb_dataframe,fab_id, firstLetterException = False):\n",
    "    pdb_id,chain,fab_type = fab_id\n",
    "    if fab_type == \"light\":\n",
    "        anarci_seq     = anarci_list_light[pdb_id.lower()+\"_\"+chain]\n",
    "    else:\n",
    "        anarci_seq     = anarci_list_heavy[pdb_id.lower()+\"_\"+chain]\n",
    "    pdb_ca        = pdb_dataframe[pdb_dataframe[\"atom_name\"] == \"CA\"]#[\"seqres\"]\n",
    "    pdb_seq = \"\".join(pdb_ca[\"seqres\"])\n",
    "    pdb_anarci_map = realign_sequences(pdb_seq, anarci_seq,firstLetterException)\n",
    "    \n",
    "    if pdb_anarci_map is None:\n",
    "        return None\n",
    "    \n",
    "    pdb_anarci_map = [fab_type[0].upper()+str(i)  if i is not None else None for i in pdb_anarci_map]    \n",
    "    pdb_dataframe[\"anarci\"] = None\n",
    "    \n",
    "    for anarci_id,residue_number in zip(pdb_anarci_map, pdb_ca[\"residue_key\"]):\n",
    "        ids = pdb_dataframe[ \"residue_key\"] == residue_number\n",
    "        pdb_dataframe.loc[ids,\"anarci\"] = anarci_id     \n",
    "        \n",
    "    return pdb_dataframe\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f\"./PDB_blast_db/structs_antibodies/\"):\n",
    "    os.mkdir(f\"./PDB_blast_db/structs_antibodies/\")        \n",
    "    \n",
    "jobs = []\n",
    "for anarci_id,anarci_map in anarci_list_heavy.items():    \n",
    "    jobs.append((anarci_id[:4].upper(),anarci_id[-1],\"heavy\"))\n",
    "for anarci_id,anarci_map in anarci_list_light.items():    \n",
    "    jobs.append((anarci_id[:4].upper(),anarci_id[-1],\"light\"))\n",
    "\n",
    "strange_error_list = set()\n",
    "for pdb_id,chain,fab_type in jobs:\n",
    "    if pdb_id.upper()+\"_\"+chain in strange_error_list:\n",
    "        continue\n",
    "    firstLetterException = True\n",
    "    print(pdb_id,chain,fab_type)\n",
    "    pdb_path           = f\"./PDB_blast_db/structs_per_chain/{pdb_id}_{chain}.pkl\"\n",
    "    if not os.path.exists(pdb_path):\n",
    "        continue\n",
    "    out_path           = f\"./PDB_blast_db/structs_antibodies/{pdb_id}_{chain}_{fab_type}.pkl\"\n",
    "    if os.path.exists(out_path):\n",
    "        continue\n",
    "    fab                = pickle.load(open(pdb_path,'rb'))\n",
    "    fab_annotated      = put_anarci_annotation(fab, (pdb_id,chain,fab_type),firstLetterException)\n",
    "    \n",
    "    ### if something went wrong, we skip this complex\n",
    "    if fab_annotated is None:\n",
    "        strange_error_list.add((pdb_id,chain,fab_type))\n",
    "        continue            \n",
    "    pickle.dump(fab_annotated, open(out_path,'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test that nothing is missing\n",
    "\n",
    "train = pd.read_csv(\"../data/sema_1.0/train_set.csv\")\n",
    "test = pd.read_csv(\"../data/sema_1.0/test_set.csv\")\n",
    "\n",
    "ref_names = [d[\"pdb_id_chain\"][:6] for d in train.iloc()]\n",
    "ref_names+= [d[\"pdb_id_chain\"][:6] for d in test.iloc()]\n",
    "ref_names = set(ref_names)    \n",
    "\n",
    "n =0 \n",
    "for p in Path(\"./PDB_blast_db/structs_per_chain/\").glob(f\"*.pkl\"):\n",
    "    if p.name[:6] in ref_names:\n",
    "        n+=1\n",
    "        \n",
    "print(n, len(ref_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Find heavy/light chain fab pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_pdb_list():\n",
    "    return [p.name[:4] for p in Path(\"./PDB_blast_db/structs_antibodies/\").glob(f\"*.pkl\")]\n",
    "    \n",
    "def get_fabs_pdbid(pdb_id = \"1LK3\"):\n",
    "    fab_path = Path(\"./PDB_blast_db/structs_antibodies/\").glob(f\"{pdb_id}*.pkl\")\n",
    "    fab_ids  = {\"heavy\":[],\"light\":[]}\n",
    "    for struct_id in fab_path:\n",
    "        pdb_id, chain, fab_type = struct_id.name.rstrip(\".pkl\").split(\"_\")\n",
    "        fab_ids[fab_type].append(struct_id)\n",
    "    return fab_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.spatial import distance\n",
    "\n",
    "def getxyz(df):\n",
    "    xyz = np.array([df[\"x_coord\"],df[\"y_coord\"],df[\"z_coord\"]]).T\n",
    "    return xyz\n",
    "    \n",
    "    \n",
    "def get_pair_interface(path_light, path_heavy):\n",
    "    \n",
    "    pdb_light = pickle.load(open(path_light,'rb'))\n",
    "    pdb_heavy = pickle.load(open(path_heavy,'rb'))\n",
    "    \n",
    "    ### interface residues of heavy and light fab chains\n",
    "    heavy_interface = list(range(32,39)) + list(range(44,50)) + list(range(85,95))\n",
    "    light_interface = list(range(34,39)) + list(range(45,51)) + list(range(90,108))\n",
    "\n",
    "    heavy_ids = [\"H\"+str(i) for i in heavy_interface]\n",
    "    light_ids = [\"L\"+str(i) for i in light_interface]\n",
    "    \n",
    "    heavy_interface = pdb_heavy[pdb_heavy[\"anarci\"].isin(heavy_ids)]\n",
    "    light_interface = pdb_light[pdb_light[\"anarci\"].isin(light_ids)]\n",
    "    \n",
    "    xyz_heavy = getxyz(heavy_interface)\n",
    "    xyz_light = getxyz(light_interface)\n",
    "    \n",
    "    cd = distance.cdist(xyz_heavy,xyz_light)\n",
    "    ids = np.where(cd<4.5)\n",
    "    \n",
    "    return len(set(ids[0]))+len(set(ids[1]))\n",
    "    \n",
    "def screen_fab_pairs(pdb_id):\n",
    "    fab_path = get_fabs_pdbid(pdb_id)\n",
    "    contacts = {}\n",
    "    for heavy_path in fab_path[\"heavy\"]:\n",
    "        for light_path in fab_path[\"light\"]:\n",
    "            n = get_pair_interface(path_light = light_path, path_heavy = heavy_path)\n",
    "            ### cut off to select interface that interact with each other\n",
    "            if n > 3:# 10:\n",
    "                contacts[(light_path.name.rstrip(\".pkl\"), heavy_path.name.rstrip(\".pkl\"))] = n\n",
    "    return contacts\n",
    "\n",
    "\n",
    "fab_contacts = {}\n",
    "for pdb_id in get_pdb_list():\n",
    "    fab_contacts[pdb_id] = screen_fab_pairs(pdb_id)\n",
    "    #print(pdb_id,fab_contacts[pdb_id])\n",
    "    #break\n",
    "pickle.dump(fab_contacts, open(\"./PDB_blast_db/fab_pairs.pkl\",'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Find antigens and corresponding interacting antibodies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_all_fabs():\n",
    "    return [l+\".pkl\" for l in fab_ids]\n",
    "\n",
    "def get_all_antigens_list():\n",
    "    fab_ids = set(anarci_list_heavy)|set(anarci_list_light)\n",
    "    fab_ids = {f[:4].upper()+\"_\"+f[-1] for f in fab_ids}#print(fab_ids)\n",
    "    pdb_ids = [p for p in Path(\"./PDB_blast_db/structs_per_chain/\").glob(\"*.pkl\") if p.name[:6] not in fab_ids]\n",
    "    return pdb_ids\n",
    "\n",
    "def get_antigens_PDBID(pdb_id = \"1LK3\"):\n",
    "    all_antigens = get_all_antigens_list()\n",
    "    return [a for a in all_antigens if a.name[:4] == pdb_id]\n",
    "\n",
    "\n",
    "def screen_antigen_contacts(pdb_id = \"1LK3\"):\n",
    "    if pdb_id not in fab_contacts:\n",
    "        return []\n",
    "    fab_pairs = fab_contacts[pdb_id]\n",
    "    antigens  = get_antigens_PDBID(pdb_id)\n",
    "    hits = []\n",
    "    for antigen in antigens:\n",
    "        for fab_pair in fab_pairs:\n",
    "            print(antigen, fab_pair)\n",
    "            fab_path_light = Path(\"./PDB_blast_db/structs_antibodies/\"+fab_pair[0]+\".pkl\")\n",
    "            fab_path_heavy = Path(\"./PDB_blast_db/structs_antibodies/\"+fab_pair[1]+\".pkl\")\n",
    "            n_light = test_contacts(antigen,fab_path_light)\n",
    "            n_heavy = test_contacts(antigen,fab_path_heavy)\n",
    "            if n_light+n_heavy == 0:\n",
    "                continue\n",
    "            hits.append({\"antigen\":antigen,\n",
    "                         \"fab_pair\":fab_pair,\n",
    "                         \"n_contacts_light\":n_light,\n",
    "                         \"n_contacts_heavy\":n_heavy})\n",
    "    return hits\n",
    "        \n",
    "def test_contacts(antigen_path, fab_path):\n",
    "    ### check if there is a contact\n",
    "    #print(antigen_path)\n",
    "    \n",
    "    antigen_df = pickle.load(antigen_path.open('rb'))\n",
    "    if antigen_df is None:\n",
    "        return 0\n",
    "    \n",
    "    fab_df     = pickle.load(fab_path.open('rb'))\n",
    "    fab_type   = None\n",
    "    interface  = None\n",
    "    \n",
    "    ### CDR1-3 residues of light chain\n",
    "    ### CDR1-3 residues of heavy chain\n",
    "    \n",
    "    if fab_path.name.split(\"_\")[-1] == \"light.pkl\":\n",
    "        fab_type  = \"light\"\n",
    "        interface = [\"L\"+str(i) for i in list(range(23,35))+list(range(66,72))+list(range(89,98))]\n",
    "\n",
    "    elif fab_path.name.split(\"_\")[-1] == \"heavy.pkl\":\n",
    "        fab_type  = \"heavy\" \n",
    "        interface = [\"H\"+str(i) for i in list(range(23,35))+list(range(51,57))+list(range(93,102))]\n",
    "    \n",
    "    fab_interface = fab_df[fab_df[\"anarci\"].isin(interface)]\n",
    "    xyz_fab       = getxyz(fab_interface)\n",
    "    xyz_antigen   = getxyz(antigen_df)     \n",
    "    c = ~np.isnan(xyz_antigen)[:,0]\n",
    "    xyz_antigen = xyz_antigen[c]\n",
    "    cd  = distance.cdist(xyz_antigen, xyz_fab)\n",
    "    ids = set(np.where(cd<4.5)[0])\n",
    "    return len(ids)\n",
    "\n",
    "hits = []\n",
    "for pdb_id in set([n.name[:4] for n in get_all_antigens_list()]):\n",
    "    hits+=screen_antigen_contacts(pdb_id)\n",
    "        \n",
    "df = pd.DataFrame(hits)\n",
    "pickle.dump(df, open(\"./PDB_blast_db/antigen_fab_list.pkl\",'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Calculate contact numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ivanisenko/projects/SEMA_augmentation_lazy/PDB_epitope_dataset_generation\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_contact_number(df,\n",
    "                             antigen_chain_id,\n",
    "                             light_id,\n",
    "                             heavy_id,\n",
    "                             R1=8.0,\n",
    "                             R2=16.0):\n",
    "    \"\"\"\n",
    "    calculate contact nubmers\n",
    "    R1 - is used to calculate contact number values\n",
    "    R>R2 - is masked\n",
    "    \"\"\"\n",
    "    df_A = df[df[\"chain_id\"]==antigen_chain_id]\n",
    "    df_B = df[df[\"chain_id\"].isin([light_id, heavy_id])]\n",
    "    xyz_fab = getxyz(df_B)\n",
    "    df.loc[:,\"b_factor\"] = -1\n",
    "    resi_ids = [\"residue_name\", \"chain_id\", \"residue_number\", \"insertion\"]\n",
    "    for resi, _df in df_A.groupby(resi_ids):\n",
    "        xyz_resi = getxyz(_df)\n",
    "        cd = distance.cdist(xyz_resi, xyz_fab)\n",
    "        ids = list(set(np.where(cd<R2)[1]))\n",
    "        if len(ids) == 0:\n",
    "            continue\n",
    "        df.loc[_df.index.values,\"b_factor\"] = 0\n",
    "    \n",
    "    for resi, _df in df_A.groupby(resi_ids):\n",
    "        xyz_resi = getxyz(_df)\n",
    "        cd = distance.cdist(xyz_resi, xyz_fab)\n",
    "        ids = list(set(np.where(cd<R1)[1]))\n",
    "        if len(ids) == 0:\n",
    "            continue\n",
    "        target = df_B.iloc()[ids]\n",
    "        n_fab_resi = len(target.groupby(resi_ids))\n",
    "        df.loc[_df.index.values,\"b_factor\"] = n_fab_resi\n",
    "        \n",
    "    return\n",
    "\n",
    "def getxyz(df):\n",
    "    xyz = np.array([df[\"x_coord\"],df[\"y_coord\"],df[\"z_coord\"]]).T\n",
    "    return xyz\n",
    "\n",
    "def load_old_ref_names():\n",
    "    \"\"\"\n",
    "    Function to get compatiblity with old train set\n",
    "    \"\"\"\n",
    "    train = pd.read_csv(\"../data/sema_1.0/train_set.csv\")\n",
    "    test = pd.read_csv(\"../data/sema_1.0/test_set.csv\")\n",
    "    ref_names = [d[\"pdb_id_chain\"][:6] for d in train.iloc()]\n",
    "    ref_names+= [d[\"pdb_id_chain\"][:6] for d in test.iloc()]\n",
    "    return set(ref_names)\n",
    "\n",
    "def get_clusters():\n",
    "    clusters = {}\n",
    "    with open(\"./PDB_blast_db/clusters_095/results_cluster.tsv\") as f:\n",
    "        for line in f:\n",
    "            r = line.rstrip().split()\n",
    "            clusters.setdefault(r[0],set())\n",
    "            clusters[r[0]].add(r[1])\n",
    "\n",
    "    clusters_h = {}\n",
    "    for ref_id, other in clusters.items():\n",
    "        for c in other:\n",
    "            clusters_h[c] = ref_id\n",
    "            \n",
    "    return clusters_h\n",
    "\n",
    "def extract_pdbs(data,\n",
    "                 output = \"./dataset/\",\n",
    "                 R1 = 8.0,\n",
    "                 R2 = 16.0):\n",
    "\n",
    "    clusters = get_clusters()\n",
    "    \n",
    "    Path(output+\"/\").mkdir(exist_ok=True)\n",
    "    Path(output+\"/clusters/\").mkdir(exist_ok=True)\n",
    "    Path(output+\"/antigen/\").mkdir(exist_ok=True)\n",
    "    Path(output+\"/antigen_fab/\").mkdir(exist_ok=True)\n",
    "    Path(output+\"/antigen_fab_raw/\").mkdir(exist_ok=True)\n",
    "    \n",
    "    for d in data.iloc():\n",
    "        antigen_chain_id = d[\"antigen\"].name.split(\"_\")[-1][0]\n",
    "        pdb_id = d[\"antigen\"].name.split(\"/\")[-1][:4]\n",
    "\n",
    "        light_id = d[\"fab_pair\"][0].split(\"_\")[1]\n",
    "        heavy_id = d[\"fab_pair\"][1].split(\"_\")[1]\n",
    "        \n",
    "        output_name = f\"{pdb_id}_{antigen_chain_id}_{light_id}_{heavy_id}.pkl\"\n",
    "        print(output_name)\n",
    "\n",
    "        if os.path.exists(output_name):\n",
    "            continue\n",
    "        \n",
    "        path = pickle.load(open(d[\"antigen\"],'rb'))\n",
    "        \n",
    "        if pdb_id+\"_\"+antigen_chain_id not in clusters:\n",
    "            continue\n",
    "            \n",
    "        ref_name = clusters[pdb_id+\"_\"+antigen_chain_id]\n",
    "        out = f\"./{output}/clusters/{ref_name}\"\n",
    "        \n",
    "        if os.path.exists(out+f\"/{pdb_id}_{antigen_chain_id}_{light_id}_{heavy_id}.pkl\"):\n",
    "            continue\n",
    "        \n",
    "        Path(out).mkdir(exist_ok=True)\n",
    "        antigen_df = pickle.load(open(d[\"antigen\"],'rb'))\n",
    "\n",
    "        pdb_structure      = PandasPdb().read_pdb(f\"./PDB_blast_db/structs/{pdb_id}.pdb\")\n",
    "        pdb_structure.df.pop(\"HETATM\")\n",
    "        pdb_structure.df[\"ATOM\"] = pdb_structure.df[\"ATOM\"][pdb_structure.df[\"ATOM\"][\"chain_id\"].isin([antigen_chain_id,\n",
    "                                                                                                       light_id,\n",
    "                                                                                                       heavy_id])]\n",
    "        pdb_structure.df[\"ATOM\"] = pdb_structure.df[\"ATOM\"][pdb_structure.df[\"ATOM\"]['alt_loc'].isin(['', 'A'])]\n",
    "        \n",
    "        calculate_contact_number(pdb_structure.df[\"ATOM\"],\n",
    "                                 antigen_chain_id,\n",
    "                                 light_id,\n",
    "                                 heavy_id,\n",
    "                                 R1 = R1,\n",
    "                                 R2 = R2)\n",
    "    \n",
    "        resi_ids = [\"residue_number\",\"insertion\", \"chain_id\", \"residue_name\"]\n",
    "        antigen_df[\"b_factor\"] = -1\n",
    "        \n",
    "        for r,d in antigen_df.groupby(resi_ids):\n",
    "            key = (int(r[0]),r[1],r[2],r[3])\n",
    "            df_ = pdb_structure.df[\"ATOM\"][(pdb_structure.df[\"ATOM\"][\"residue_number\"] == int(r[0])) &\n",
    "            (pdb_structure.df[\"ATOM\"][\"insertion\"] == r[1]) &\n",
    "            (pdb_structure.df[\"ATOM\"][\"chain_id\"] == r[2]) &\n",
    "            (pdb_structure.df[\"ATOM\"][\"residue_name\"] == r[3])]\n",
    "            if df_.shape[0]==0:\n",
    "                continue\n",
    "            antigen_df.loc[d.index.values, \"b_factor\"] = df_.iloc()[0][\"b_factor\"]\n",
    "\n",
    "        pickle.dump(antigen_df, open(out+f\"/{pdb_id}_{antigen_chain_id}_{light_id}_{heavy_id}.pkl\",'wb'))\n",
    "        \n",
    "        antigen_df = antigen_df.dropna(subset=['residue_number'])        \n",
    "        antigen_df['residue_number'] = antigen_df['residue_number'].astype(int)\n",
    "        antigen_df['atom_number'] = antigen_df['atom_number'].astype(int)\n",
    "        \n",
    "        pdb_structure.df[\"ATOM\"] = antigen_df\n",
    "        pdb_structure.to_pdb(out+f\"/{pdb_id}_{antigen_chain_id}_{light_id}_{heavy_id}.pdb\")\n",
    "\n",
    "        print(out+f\"/{pdb_id}_{antigen_chain_id}_{light_id}_{heavy_id}.pdb\")\n",
    "        \n",
    "data = pickle.load(open(\"./PDB_blast_db/antigen_fab_list.pkl\",'rb'))        \n",
    "extract_pdbs(data, output=\"./dataset_4.5/\", R1=4.5, R2=16.0)\n",
    "\n",
    "data = pickle.load(open(\"./PDB_blast_db/antigen_fab_list.pkl\",'rb'))\n",
    "extract_pdbs(data, output=\"./dataset/\",     R1=8.0, R2=16.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Collect dataset of homologoues antigen clusters with calculated contact number values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_contacts(antigen_ids, fab_pair):\n",
    "    \"\"\"\n",
    "    Function to extract multimers data\n",
    "    \"\"\"\n",
    "    \n",
    "    multimer_list = []\n",
    "    coords = []\n",
    "    for antigen_id in antigen_ids:\n",
    "        prot = pickle.load(antigen_id.open('rb'))\n",
    "        prot = prot.dropna(subset=['residue_number'])\n",
    "        coords.append(prot[[\"x_coord\",\"y_coord\",\"z_coord\"]].to_numpy())\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    \n",
    "    for i in range(len(coords)):\n",
    "        for j in range(i+1, len(coords)):\n",
    "            cd = distance.cdist(coords[i], coords[j])\n",
    "            n  = len(set(np.where(cd<4.5)[0]))\n",
    "            if n>10:\n",
    "                G.add_edge(i,j)\n",
    "    if len(G.edges())==0:\n",
    "        return\n",
    "    \n",
    "    connected_components = nx.connected_components(G)\n",
    "    r = list((comp for comp in connected_components if 0 in comp))\n",
    "    \n",
    "    if len(r) == 0:\n",
    "        return\n",
    "    \n",
    "    longest_subgraph_nodes = max(r, key=len)\n",
    "    longest_subgraph = G.subgraph(longest_subgraph_nodes)\n",
    "\n",
    "    antigen_multimer = [antigen_ids[i].name[:-4] for i in longest_subgraph]\n",
    "    \n",
    "    if len(antigen_multimer) <= 1:\n",
    "        return\n",
    "    \n",
    "    fab_chains = [c.split(\"_\")[1] for c in fab_pair]\n",
    "    all_paths = []\n",
    "    \n",
    "    for a in antigen_multimer:\n",
    "        all_paths += list(Path(\"./dataset/clusters/\").glob(\"*/\"+a+\"*.pkl\"))\n",
    "    \n",
    "    if len(all_paths) == 0:\n",
    "        return\n",
    "    \n",
    "    prot_per_chain = {}\n",
    "    pid = all_paths[0].name[:4]\n",
    "    for path in all_paths:\n",
    "        pdb_id = path.name[:6]        \n",
    "        prot_per_chain.setdefault(pdb_id, [])\n",
    "        prot_per_chain[pdb_id].append(pickle.load(path.open('rb')))\n",
    "    \n",
    "    multimer_prot = []\n",
    "    for pdb_id in prot_per_chain:\n",
    "        ref_pickle = prot_per_chain[pdb_id][0]\n",
    "        bs = [p[\"b_factor\"].to_numpy() for p in prot_per_chain[pdb_id]]\n",
    "        bs = np.array(bs)\n",
    "        bs = np.max(bs,axis=0)\n",
    "        ref_pickle[\"b_factor\"] = bs\n",
    "        multimer_prot.append(ref_pickle)\n",
    "    multimer_prot = pd.concat(multimer_prot)\n",
    "    if np.max(bs)<=0:\n",
    "        return\n",
    "    \n",
    "    name = pid\n",
    "\n",
    "    for c in sorted(list(set(list(multimer_prot.dropna(subset=[\"residue_number\"])[\"chain_id\"])))):\n",
    "        name+=\"_\"+c\n",
    "    \n",
    "    pdb = PandasPdb()\n",
    "    pdb.df[\"ATOM\"] = multimer_prot.dropna(subset=[\"residue_number\"])\n",
    "\n",
    "    for c in [\"atom_number\", \"residue_number\", \"line_idx\"]:\n",
    "        pdb.df[\"ATOM\"][c] = pdb.df[\"ATOM\"][c].astype(int)\n",
    "\n",
    "    Path(\"./dataset/multimers/\").mkdir(exist_ok=True)\n",
    "    pdb.to_pdb(\"./dataset/multimers/\"+name+\".pdb\")\n",
    "    return\n",
    "    \n",
    "def find_proteins():\n",
    "    \"\"\"\n",
    "    Function to prepare multimers dataset\n",
    "    \"\"\"\n",
    "    fab_list = pickle.load(open(\"./PDB_blast_db/antigen_fab_list.pkl\",'rb'))\n",
    "    for name, d_ in fab_list.groupby([\"fab_pair\"]):\n",
    "        if d_.shape[0] == 1:\n",
    "            continue\n",
    "            \n",
    "        q = [p.name[:-4] for p in list(d_[\"antigen\"])]\n",
    "\n",
    "        pdb_id = q[0].split(\"_\")[0]\n",
    "        chain_ids = [q_.split(\"_\")[1] for q_ in q]\n",
    "        antigen_ids = list(d_[\"antigen\"])\n",
    "       \n",
    "        nc1 = np.array(d_[\"n_contacts_heavy\"])\n",
    "        nc1+= np.array(d_[\"n_contacts_light\"])\n",
    "        \n",
    "        ids = list(range(len(nc1)))\n",
    "        ids.sort(key = nc1.__getitem__)\n",
    "        ids = list(reversed(ids))\n",
    "        \n",
    "        chain_ids =  [chain_ids[i] for i in ids]\n",
    "        antigen_ids = [antigen_ids[i] for i in ids]\n",
    "\n",
    "        find_contacts(antigen_ids, name[0])\n",
    "        print(\"-------\")\n",
    "        #exit(0)\n",
    "\n",
    "\n",
    "def cluster_data(name = \"5I8H_C\", ref_cluster_name = None, folder_name = \"dataset\"):\n",
    "    \"\"\"\n",
    "    Calculate MSA for each cluster to calculate consensus contact number values\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    names = []\n",
    "    dfs = {}\n",
    "    dfs_full = {}\n",
    "    \n",
    "    fo = open(f\"./{folder_name}/clusters/\"+name+\"/msa.fasta\",'w')\n",
    "    for path in Path(f\"./{folder_name}/clusters/\"+name+\"/\").glob(\"*.pkl\"):\n",
    "        data = pickle.load(path.open('rb'))\n",
    "        data_ca = data[data[\"atom_name\"]==\"CA\"].reset_index(drop=True)\n",
    "        sequences.append(\"\".join(data_ca[\"seqres\"]))\n",
    "        names.append(path.name)\n",
    "        dfs[path.name.split(\".\")[0]] = data_ca\n",
    "        dfs_full[path.name.split(\".\")[0]] = data            \n",
    "        fo.write(f\">{names[-1]}\\n\")\n",
    "        fo.write(f\"{sequences[-1]}\\n\")\n",
    "    fo.close()\n",
    "    \n",
    "    if len(sequences)>1:\n",
    "        subprocess.call(f\"mafft --anysymbol --auto ./{folder_name}/clusters/\"+name+f\"/msa.fasta > ./{folder_name}/clusters/\"+name+\"/msa_al.fasta\",shell=True)\n",
    "    else:\n",
    "        subprocess.call(f\"cp ./{folder_name}/clusters/\"+name+f\"/msa.fasta ./{folder_name}/clusters/\"+name+\"/msa_al.fasta\",shell=True)\n",
    "\n",
    "    seqs = []\n",
    "    Path(f\"./{folder_name}/consensus/\").mkdir(exist_ok=True)\n",
    "    with open(f\"./{folder_name}/clusters/\"+name+\"/msa_al.fasta\") as f:\n",
    "        for line in f:\n",
    "            if line[0]==\">\":\n",
    "                seqs.append([])\n",
    "            seqs[-1].append(line.rstrip())\n",
    "    \n",
    "    seqs_h = {}\n",
    "    if ref_cluster_name is None:\n",
    "        ref_cluster_name = name\n",
    "        \n",
    "    ref_name = None\n",
    "    for seq in seqs:\n",
    "        n = seq[0][1:]\n",
    "        seqs_h[n] = \"\".join(seq[1:]).replace(\"\\n\",\"\")\n",
    "        if n.startswith(ref_cluster_name):\n",
    "            ref_name = n\n",
    "\n",
    "    ref_seq = seqs_h[ref_name]\n",
    "    contacts = {}\n",
    "    \n",
    "    if len(ref_seq.replace(\"-\",\"\")) != len(dfs[ref_name[:-4]]):\n",
    "        return\n",
    "    \n",
    "    for n,s in seqs_h.items():\n",
    "        n1 = 0\n",
    "        n2 = 0\n",
    "        for s1,s2 in zip(ref_seq,s):\n",
    "            if s1!=\"-\":\n",
    "                n1+=1\n",
    "            if s2!=\"-\":\n",
    "                n2+=1\n",
    "            if s1!=\"-\" and s2!=\"-\":\n",
    "                contacts.setdefault(n1-1, [])\n",
    "                contacts[n1-1].append([n,n2-1])\n",
    "\n",
    "    consensus_cn = {}\n",
    "    dfs[ref_name[:-4]][\"consensus_contact_number\"] = -1\n",
    "    \n",
    "    for i in contacts:    \n",
    "        ref_aa = dfs[ref_name.split(\".\")[0]].iloc()[i][\"seqres\"]\n",
    "        tar_aas = []\n",
    "        tar_b = []\n",
    "        for c,ii in contacts[i]:\n",
    "            c = c.split(\".pkl\")[0]\n",
    "            tar_aa = dfs[c].iloc()[ii][\"seqres\"]\n",
    "            print(tar_aa,ref_aa)\n",
    "            if tar_aa != ref_aa:\n",
    "                print(\"Error!\")\n",
    "                continue\n",
    "            tar_aas.append(tar_aa)\n",
    "            tar_b.append(dfs[c].iloc()[ii][\"b_factor\"])\n",
    "        l = list(set(tar_aas))\n",
    "        consensus_cn[i] = np.max(tar_b)\n",
    "        dfs[ref_name[:-4]].loc[i, \"consensus_contact_number\"] = np.max(tar_b)\n",
    "\n",
    "    ref_df = dfs[ref_name[:-4]]\n",
    "    ref_df_full = dfs_full[ref_name[:-4]]\n",
    "    for k,d in ref_df_full.groupby([\"residue_key\"]):\n",
    "        cn = ref_df[ref_df[\"residue_key\"] == k[0]].iloc()[0][\"consensus_contact_number\"]\n",
    "        ref_df_full.loc[ref_df_full[\"residue_key\"] == k[0], \"b_factor\"] = cn\n",
    "\n",
    "    pickle.dump(ref_df, open(f\"./{folder_name}/consensus/\"+ref_name+\"_CA.pkl\",'wb'))\n",
    "    pickle.dump(ref_df_full, open(f\"./{folder_name}/consensus/\"+ref_name+\"_full.pkl\",'wb'))\n",
    "    prot = PandasPdb()\n",
    "    \n",
    "    prot.df[\"ATOM\"] = ref_df_full.dropna(subset=['atom_number'])\n",
    "    \n",
    "    for c in [\"atom_number\", \"residue_number\", \"line_idx\"]:\n",
    "        prot.df[\"ATOM\"][c] = prot.df[\"ATOM\"][c].astype(int)\n",
    "    \n",
    "    prot.to_pdb(f\"./{folder_name}/consensus/\"+ref_name+\".pdb\")\n",
    "    \n",
    "\n",
    "def load_old_ref_names():\n",
    "    \"\"\"\n",
    "    Select compatible reference cluster names \n",
    "    \"\"\"\n",
    "    train = pd.read_csv(\"../data/sema_1.0/train_set.csv\")\n",
    "    test = pd.read_csv(\"../data/sema_1.0/test_set.csv\")\n",
    "    ref_names = [d[\"pdb_id_chain\"][:6] for d in train.iloc()]\n",
    "    ref_names+= [d[\"pdb_id_chain\"][:6] for d in  test.iloc()]\n",
    "    return set(ref_names)\n",
    "\n",
    "def prepare_ds():\n",
    "    \"\"\"\n",
    "    Function to calculate consensus contact number values aggregating precalculated antigen/fab structures above    \n",
    "    \"\"\"\n",
    "    old_ref_names = load_old_ref_names()\n",
    "    fab_list = pickle.load(open(\"./PDB_blast_db/antigen_fab_list.pkl\",'rb'))\n",
    "    n = set([n.name[:-4] for n in fab_list[\"antigen\"]])\n",
    "    \n",
    "    old_ref_names = load_old_ref_names()\n",
    "    \n",
    "    for ds_name in [\"_4.5\", \"\"]:\n",
    "        test = set()\n",
    "        \n",
    "        for path in Path(f\"./dataset{ds_name}/clusters/\").glob(\"*\"):\n",
    "            ref_name = path.name[:6]\n",
    "            \n",
    "            if len(path.name)!=6:\n",
    "                continue\n",
    "                \n",
    "            for p in Path(f\"./dataset{ds_name}/clusters/\"+path.name+\"/\").glob(\"*.pkl\"):\n",
    "                pot_ref_name = p.name[:6]\n",
    "                if pot_ref_name in old_ref_names:\n",
    "                    ref_name = pot_ref_name\n",
    "                    test.add(ref_name)\n",
    "            \n",
    "            print(path.name, ref_name, path.name == ref_name, len(test), len(old_ref_names))\n",
    "            cluster_data(path.name, ref_name, f\"dataset{ds_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_ds()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Prepare train and test csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def homology_filter_epi(df_train, df_test, p_cut): \n",
    "    \"\"\"\n",
    "    Function that filters out sequences from the train set that have homology to the test set\n",
    "    \"\"\"\n",
    "    seq_ids1 = {\"pdb_id_chain\":[],\n",
    "               \"wt_seq\":[]}\n",
    "    \n",
    "    for k,d in df_train.groupby([\"pdb_id_chain\"]):\n",
    "        seq_ids1[\"pdb_id_chain\"].append(k[0])\n",
    "        seq = \"\".join(list(d[\"resi_aa\"]))\n",
    "        seq_ids1[\"wt_seq\"].append(seq)\n",
    "\n",
    "    seq_ids2 = {\"pdb_id_chain\":[],\n",
    "               \"wt_seq\":[]}\n",
    "    \n",
    "    for k,d in df_test.groupby([\"pdb_id_chain\"]):\n",
    "        seq_ids2[\"pdb_id_chain\"].append(k[0])\n",
    "        seq = \"\".join(list(d[\"resi_aa\"]))\n",
    "        seq_ids2[\"wt_seq\"].append(seq)\n",
    "\n",
    "    hf = homology_filter(pd.DataFrame(seq_ids1), pd.DataFrame(seq_ids2), p_cut=p_cut)\n",
    "    df_train_filtered = df_train[df_train[\"pdb_id_chain\"].isin(hf[\"pdb_id_chain\"])]\n",
    "    print(df_train.shape, df_train_filtered.shape)\n",
    "    return df_train_filtered\n",
    "          \n",
    "         \n",
    "\n",
    "def homology_filter(df_train, \n",
    "                    df_test,\n",
    "                    p_cut):\n",
    "\n",
    "    used = set()\n",
    "    name = \"test_\"\n",
    "    subprocess.call(\"rm -r temp\",shell=True)\n",
    "    Path(\"./temp/\").mkdir(exist_ok=True)\n",
    "    with open(f\"./temp/all_sequences_{name}.fasta\",'w') as fo:\n",
    "        for d_ in df_train.iloc():\n",
    "            if d_['pdb_id_chain'] in used:\n",
    "                continue\n",
    "            fo.write(f\">{d_['pdb_id_chain']}\\n{d_['wt_seq']}\\n\")\n",
    "            used.add(d_['pdb_id_chain'])\n",
    "    used = set()\n",
    "    with open(f\"./temp/test_sequences_{name}.fasta\",'w') as fo_test:\n",
    "        for d_ in df_test.iloc():\n",
    "            if d_['pdb_id_chain'] in used:\n",
    "                continue\n",
    "            fo_test.write(f\">{d_['pdb_id_chain']}\\n{d_['wt_seq']}\\n\")\n",
    "            used.add(d_['pdb_id_chain'])\n",
    "            \n",
    "    subprocess.call(f\"makeblastdb -in all_sequences_{name}.fasta -dbtype prot\", shell=True, cwd=\"./temp/\")\n",
    "    subprocess.call(f\"blastp      -db all_sequences_{name}.fasta -query  test_sequences_{name}.fasta -outfmt 6 -out hits_{name}.tsv -num_threads 4\", shell=True, cwd=\"./temp/\")\n",
    "    \n",
    "    hit_data    = pd.read_csv(f\"./temp/hits_{name}.tsv\", delimiter='\\t', header=None)\n",
    "    hit_data    = hit_data[(hit_data.iloc[:,2]>p_cut) & (hit_data.iloc[:,-2]<0.05)]\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    for h in hit_data.iloc():\n",
    "        e = [(h.iloc()[0],h.iloc()[1])]\n",
    "        G.add_edges_from(e)\n",
    "    subcomponents = list(nx.connected_components(G))    \n",
    "    all_skip_nodes = set()\n",
    "    for s in subcomponents:\n",
    "        all_skip_nodes|=set(s)\n",
    "    n_before = df_train.shape\n",
    "    df_train = df_train[~df_train[\"pdb_id_chain\"].isin(all_skip_nodes)]\n",
    "    n_after  = df_train.shape\n",
    "    \n",
    "    print(\"Size before filtering:\",  n_before)\n",
    "    print(\"Size after filtering:\",   n_after)\n",
    "    \n",
    "    return df_train\n",
    "\n",
    "\n",
    "def collect_all_data():\n",
    "    \"\"\"\n",
    "    Combine datasets calculated for R1=8.0/R2=16.0 and R1=4.5/R2=16.0\n",
    "    \"\"\"\n",
    "    all_dataset = {}\n",
    "    for path in Path(\"./dataset/consensus/\").glob(\"*_CA.pkl\"):\n",
    "        name = path.name[:6]\n",
    "        print(name)\n",
    "        paths_2 = list(Path(\"./dataset_4.5/consensus/\").glob(name+\"*_CA.pkl\"))[0]\n",
    "        all_dataset[name] = merge_big_small(path, paths_2)\n",
    "    pickle.dump(all_dataset, open(\"all_dataset.pkl\",'wb'))\n",
    "    \n",
    "    \n",
    "def merge_big_small(path_big, path_small):\n",
    "    \"\"\"\n",
    "    Contact numbers with R1=8.0/R2=16.0 were calculated to train the model\n",
    "    Contact nubmers with R1=4.5/R2=16.0 were calculated for binary classification\n",
    "    this function merges them     \n",
    "    \"\"\"\n",
    "    data_big = pickle.load(path_big.open('rb'))\n",
    "    data_small = pickle.load(path_small.open('rb'))\n",
    "    assert data_big.shape[0] == data_small.shape[0]\n",
    "    cn_small = data_small[\"consensus_contact_number\"]\n",
    "    data_big[\"consensus_contact_number_4.5\"] = cn_small\n",
    "    binary = []\n",
    "    for c in list(cn_small):\n",
    "        #print(c)\n",
    "        if c == -1:\n",
    "            binary.append(-1)\n",
    "            continue\n",
    "        if c == 0:\n",
    "            binary.append(0)\n",
    "        if c>0:\n",
    "            binary.append(1)    \n",
    "    data_big[\"contact_number_binary\"] = binary\n",
    "    return data_big\n",
    "\n",
    "    \n",
    "def reformat_dataframe(df, name):\n",
    "    \"\"\"\n",
    "    Change dataset to the old format\n",
    "    \"\"\"\n",
    "    data = {\"pdb_id_chain\":[],\n",
    "            \"pdb_id\":[],\n",
    "            \"resi_pos\":[],\n",
    "            \"resi_aa\":[],\n",
    "            \"resi_name\":[],\n",
    "            \"contact_number\":[],\n",
    "            \"contact_number_binary\":[]}\n",
    "        \n",
    "    for d_ in df.iloc():\n",
    "        data[\"pdb_id_chain\"].append(name)\n",
    "        data[\"pdb_id\"].append(name[:4])\n",
    "        resi_pos = d_[\"residue_number\"]\n",
    "        if not np.isnan(resi_pos):\n",
    "            resi_pos = int(resi_pos)\n",
    "        data[\"resi_pos\"].append(resi_pos)\n",
    "        data[\"resi_aa\"].append(d_[\"seqres\"])\n",
    "        data[\"resi_name\"].append(d_[\"residue_name\"])\n",
    "        cn = d_[\"b_factor\"]\n",
    "        if cn<0:\n",
    "            cn = -100\n",
    "        if cn<=0:\n",
    "            data[\"contact_number\"].append(cn)\n",
    "        else:\n",
    "            data[\"contact_number\"].append(np.log(cn+1))\n",
    "        nbin = d_[\"contact_number_binary\"]\n",
    "        if nbin<0:\n",
    "            nbin = -100\n",
    "        data[\"contact_number_binary\"].append(nbin)\n",
    "    df=  pd.DataFrame(data)\n",
    "    df[\"resi_pos\"] = df[\"resi_pos\"].astype(\"Int64\")\n",
    "    return df\n",
    "\n",
    "def generate_csv():\n",
    "    \"\"\"\n",
    "    Generates train set\n",
    "    \"\"\"\n",
    "    new_ds = pickle.load(open(\"all_dataset.pkl\",'rb'))\n",
    "    \n",
    "    old_test_updated = []\n",
    "    n = 0\n",
    "    new_test = {}\n",
    "    \n",
    "    for pid,d_ in old_ds.groupby([\"pdb_id_chain\"]):\n",
    "        pid = pid[0][:6]\n",
    "        n+=1\n",
    "        if pid not in new_ds:\n",
    "            d_add = d_\n",
    "            new_cn = []\n",
    "            new_bin = []\n",
    "            for l in d_add.iloc():\n",
    "                key = (l[\"pdb_id_chain\"][:6], int(l[\"resi_pos\"]),l[\"resi_name\"])\n",
    "                bin_val = vals_upd[key]\n",
    "                if bin_val<0:\n",
    "                    bin_val = -1\n",
    "                new_bin.append(vals_upd[key])\n",
    "                cn = l[\"contact_number\"]\n",
    "                if vals_upd[key]<0:\n",
    "                    cn = -1\n",
    "                new_cn.append(cn)\n",
    "            d_[\"contact_number\"] = new_cn\n",
    "            d_[\"contact_number_binary\"] = new_bin\n",
    "            new_test[pid] = d_\n",
    "            continue\n",
    "\n",
    "        no_nans = new_ds[pid][~new_ds[pid][\"record_name\"].isnull()]\n",
    "        new_test[pid] = reformat_dataframe(no_nans, pid)\n",
    "    \n",
    "    old_test_set = [v for k,v in new_test.items()]\n",
    "    old_test_set = pd.concat(old_test_set).reset_index(drop=True)\n",
    "    old_test_set.to_csv(\"../data/sema_1.0/test_set.csv\")\n",
    "\n",
    "    full_train_set = []\n",
    "    for pid in new_ds:\n",
    "        if pid in new_test:\n",
    "            continue\n",
    "        full_train_set.append(reformat_dataframe(new_ds[pid], pid))\n",
    "\n",
    "    full_train_set = pd.concat(full_train_set).reset_index(drop=True)\n",
    "    full_train_set_filtered = homology_filter_epi(full_train_set, old_test_set, 30.0)\n",
    "    full_train_set_filtered.to_csv(\"../data/sema_2.0/train_set.csv\")\n",
    "    \n",
    "generate_csv()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sema_env_test",
   "language": "python",
   "name": "sema_env_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
